\contentsline {section}{\numberline {1}Data Sources}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Datasets Used}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Dementia Output}{4}{subsection.1.2}%
\contentsline {section}{\numberline {2}Split Into numeric and textual}{5}{section.2}%
\contentsline {section}{\numberline {3}Numeric Features}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Informant and High Percentage of Missing}{6}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Nested Questions and High percentage of missing}{6}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Treating Features with High Percentage of Missing}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Detecting Erroneous Values Inside Features}{7}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Working with Legal Features}{8}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Filtering Legal Features with Erroneous Values - Erroneous Code-Book}{8}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Detecting Outliers}{8}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Outlier Detection and Scaling}{9}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Erroneous Codebook}{9}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Treating Erroneous Values}{10}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Data Preprocessing}{10}{subsection.3.11}%
\contentsline {subsubsection}{\numberline {3.11.1}Imputing Missing Values}{10}{subsubsection.3.11.1}%
\contentsline {subsubsection}{\numberline {3.11.2}Scaling Features}{11}{subsubsection.3.11.2}%
\contentsline {section}{\numberline {4}Feature Selection}{11}{section.4}%
\contentsline {subsection}{\numberline {4.1}Decision Tree Feature Importance}{11}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Select K Best Features}{11}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Deciding "K"}{11}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Selecting K Best Features}{11}{subsubsection.4.2.2}%
\contentsline {section}{\numberline {5}Oversampling and Undersampling}{11}{section.5}%
\contentsline {subsection}{\numberline {5.1}Dealing With Mixed Variables}{12}{subsection.5.1}%
\contentsline {section}{\numberline {6}Cost-Sensitive Learning}{12}{section.6}%
\contentsline {subsection}{\numberline {6.1}Cost sensitive learning consists of the following steps:}{12}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Tested sampling methods (using SMOTENC's sampling\_strategy options):}{13}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Tested class\_weights: (costs for each label), combination of suggestions from book and built-in options:}{13}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Probabilistic Models}{14}{subsection.6.4}%
\contentsline {subsubsection}{\numberline {6.4.1}Grid Search For Optimal Probability Threshold}{14}{subsubsection.6.4.1}%
\contentsline {subsubsection}{\numberline {6.4.2}Calibrated Models}{14}{subsubsection.6.4.2}%
\contentsline {subsubsection}{\numberline {6.4.3}Balanced Random Forest Results}{14}{subsubsection.6.4.3}%
\contentsline {subsection}{\numberline {6.5}Results - Probabilistic}{14}{subsection.6.5}%
\contentsline {subsection}{\numberline {6.6}Results - Classification}{17}{subsection.6.6}%
\contentsline {subsection}{\numberline {6.7}Results - One Class Classification}{17}{subsection.6.7}%
\contentsline {section}{\numberline {7}Encoding Categorical Data}{18}{section.7}%
\contentsline {section}{\numberline {8}SHAP}{20}{section.8}%
\contentsline {subsection}{\numberline {8.1}Global SHAP Summary Values Results}{20}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Global SHAP Interaction Values}{23}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Multi-output Decision Plot for properly classified Rows}{24}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Decision Plot for Miss-classified Row}{26}{subsection.8.4}%
\contentsline {section}{\numberline {9}Chapter 6: Precision, Recall, and F-measure}{26}{section.9}%
\contentsline {section}{\numberline {10}Chapter 7:ROC Curves and Precision-Recall Curves}{26}{section.10}%
\contentsline {subsection}{\numberline {10.1}ROC Curve}{26}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}AUC}{27}{subsection.10.2}%
\contentsline {section}{\numberline {11}Chapter 8: Probability Scoring Methods}{27}{section.11}%
\contentsline {subsection}{\numberline {11.1}Probability Metrics}{27}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}LogLoss Score}{27}{subsection.11.2}%
\contentsline {section}{\numberline {12}Cross Validation for Imbalanced Datasets}{27}{section.12}%
\contentsline {section}{\numberline {13}Chapter 10}{27}{section.13}%
\contentsline {section}{\numberline {14}Chapter 12}{28}{section.14}%
\contentsline {subsection}{\numberline {14.1}Oversampling}{28}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Undersampling Techniques}{28}{subsection.14.2}%
\contentsline {section}{\numberline {15}Data Transforms (Hiyam)}{28}{section.15}%
\contentsline {subsection}{\numberline {15.1}Scaling Numeric Data (chapter 17)}{28}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Scaling Data with Outliers (chapter 18)}{28}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}How to Encode Categorical Data (chapter 19)}{29}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}How to Make Distributions Look More Gaussian (chapter 20)}{30}{subsection.15.4}%
\contentsline {section}{\numberline {16}Data Pre-Pocessing}{30}{section.16}%
\contentsline {subsection}{\numberline {16.1}Imputing Missing Values - Categorical Features}{30}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}Imputing Missing Values - Numerical/Ordinal Features}{30}{subsection.16.2}%
\contentsline {subsection}{\numberline {16.3}Scaling - Numeric/Ordinal Features}{31}{subsection.16.3}%
\contentsline {subsection}{\numberline {16.4}Scaling - Categorical Features}{31}{subsection.16.4}%
\contentsline {section}{\numberline {17}Imputing Numerical Missing Data}{31}{section.17}%
\contentsline {subsection}{\numberline {17.1}Statistical Imputation}{31}{subsection.17.1}%
\contentsline {subsection}{\numberline {17.2}K Nearest Neighbors}{31}{subsection.17.2}%
\contentsline {subsection}{\numberline {17.3}Iterative Imputation}{31}{subsection.17.3}%
\contentsline {section}{\numberline {18}Imputing Categorical Missing Data}{32}{section.18}%
\contentsline {subsection}{\numberline {18.1}Mean Imputation}{32}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Imputation Using Most Frequent or (Zero/Constant Values)}{32}{subsection.18.2}%
\contentsline {subsection}{\numberline {18.3}Create a New Category (Random Category) for NAN Values}{32}{subsection.18.3}%
\contentsline {subsection}{\numberline {18.4} Adding a Variable To Capture NAN}{32}{subsection.18.4}%
\contentsline {subsection}{\numberline {18.5}Imputation Using Deep Learning (Datawig)}{32}{subsection.18.5}%
\contentsline {subsection}{\numberline {18.6}Regression Imputation}{32}{subsection.18.6}%
\contentsline {subsection}{\numberline {18.7}Imputation Using Interpolation}{33}{subsection.18.7}%
\contentsline {subsection}{\numberline {18.8}Multivariate Normal Imputation (MNVI)}{33}{subsection.18.8}%
\contentsline {subsection}{\numberline {18.9}Multiple Imputation by Chained Equations (MICE)}{33}{subsection.18.9}%
\contentsline {subsection}{\numberline {18.10}Stochastic regression imputation:}{34}{subsection.18.10}%
\contentsline {subsection}{\numberline {18.11}Extrapolation and Interpolation:}{34}{subsection.18.11}%
\contentsline {subsection}{\numberline {18.12}Hot-Deck Imputation}{34}{subsection.18.12}%
\contentsline {subsection}{\numberline {18.13}Some papers for handling missing categorical data (did not read them at length -- just the abstract)}{34}{subsection.18.13}%
\contentsline {section}{\numberline {19}Plan for the Paper}{34}{section.19}%
\contentsline {section}{\numberline {20}MAML - Sampling Training/Testing Tasks}{35}{section.20}%
\contentsline {section}{\numberline {21}MAML vs Shallow Models - Dementia Dataset}{37}{section.21}%
\contentsline {subsection}{\numberline {21.1}Cost Sensitive Learning in Neural Networks}{37}{subsection.21.1}%
\contentsline {subsection}{\numberline {21.2}Hyper Parameters}{38}{subsection.21.2}%
\contentsline {subsection}{\numberline {21.3}Winning Hyper Parameters}{39}{subsection.21.3}%
\contentsline {subsection}{\numberline {21.4}Quantitative Results - Without FP Growth}{42}{subsection.21.4}%
\contentsline {subsection}{\numberline {21.5}Insights}{48}{subsection.21.5}%
\contentsline {subsection}{\numberline {21.6}Quantitative Results - With FP Growth}{49}{subsection.21.6}%
\contentsline {subsection}{\numberline {21.7}Insights}{51}{subsection.21.7}%
\contentsline {subsection}{\numberline {21.8}Insights on With FP vs. Without FP}{51}{subsection.21.8}%
\contentsline {subsection}{\numberline {21.9}Insights about PPV}{51}{subsection.21.9}%
\contentsline {subsection}{\numberline {21.10}Qualitative Results}{52}{subsection.21.10}%
\contentsline {subsubsection}{\numberline {21.10.1}Mean Empirical Risk Curves}{52}{subsubsection.21.10.1}%
\contentsline {subsubsection}{\numberline {21.10.2}Precision at Top K}{54}{subsubsection.21.10.2}%
\contentsline {subsubsection}{\numberline {21.10.3}Recall at Top K}{56}{subsubsection.21.10.3}%
\contentsline {subsubsection}{\numberline {21.10.4}ROC Curves}{57}{subsubsection.21.10.4}%
\contentsline {section}{\numberline {22}ARML - Dementia}{59}{section.22}%
\contentsline {subsection}{\numberline {22.1}Quantitative Results - Dementia Top 10 Features}{59}{subsection.22.1}%
\contentsline {subsection}{\numberline {22.2}Quantitative Results - Dementia Top 20 Features}{62}{subsection.22.2}%
\contentsline {subsection}{\numberline {22.3}Winning Hyper Parameters}{65}{subsection.22.3}%
\contentsline {subsection}{\numberline {22.4}Qualitative Results}{66}{subsection.22.4}%
\contentsline {subsubsection}{\numberline {22.4.1}Mean Empirical Risk Curves}{66}{subsubsection.22.4.1}%
\contentsline {subsubsection}{\numberline {22.4.2}Precision at Top K}{67}{subsubsection.22.4.2}%
\contentsline {subsubsection}{\numberline {22.4.3}Recall at Top K}{68}{subsubsection.22.4.3}%
\contentsline {subsubsection}{\numberline {22.4.4}ROC Curves}{68}{subsubsection.22.4.4}%
\contentsline {subsection}{\numberline {22.5}Frequent Patterns and Probability of Mistakes}{69}{subsection.22.5}%
\contentsline {section}{\numberline {23}All Results - Dementia - Quantitative}{69}{section.23}%
\contentsline {section}{\numberline {24}Reptile}{69}{section.24}%
\contentsline {subsection}{\numberline {24.1}Hyper Parameters}{69}{subsection.24.1}%
