\contentsline {section}{\numberline {1}Data Sources}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Datasets Used}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Dementia Output}{5}{subsection.1.2}%
\contentsline {section}{\numberline {2}Split Into numeric and textual}{5}{section.2}%
\contentsline {section}{\numberline {3}Numeric Features}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Informant and High Percentage of Missing}{6}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Nested Questions and High percentage of missing}{7}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Treating Features with High Percentage of Missing}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Detecting Erroneous Values Inside Features}{8}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Working with Legal Features}{8}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Filtering Legal Features with Erroneous Values - Erroneous Code-Book}{8}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Detecting Outliers}{9}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Outlier Detection and Scaling}{10}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Erroneous Codebook}{10}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Treating Erroneous Values}{11}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Data Preprocessing}{11}{subsection.3.11}%
\contentsline {subsubsection}{\numberline {3.11.1}Imputing Missing Values}{11}{subsubsection.3.11.1}%
\contentsline {subsubsection}{\numberline {3.11.2}Scaling Features}{11}{subsubsection.3.11.2}%
\contentsline {section}{\numberline {4}Feature Selection}{11}{section.4}%
\contentsline {subsection}{\numberline {4.1}Decision Tree Feature Importance}{11}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Select K Best Features}{12}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Deciding "K"}{12}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Selecting K Best Features}{12}{subsubsection.4.2.2}%
\contentsline {section}{\numberline {5}Oversampling and Undersampling}{12}{section.5}%
\contentsline {subsection}{\numberline {5.1}Dealing With Mixed Variables}{12}{subsection.5.1}%
\contentsline {section}{\numberline {6}Cost-Sensitive Learning}{13}{section.6}%
\contentsline {subsection}{\numberline {6.1}Cost sensitive learning consists of the following steps:}{13}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Tested sampling methods (using SMOTENC's sampling\_strategy options):}{13}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Tested class\_weights: (costs for each label), combination of suggestions from book and built-in options:}{14}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Probabilistic Models}{14}{subsection.6.4}%
\contentsline {subsubsection}{\numberline {6.4.1}Grid Search For Optimal Probability Threshold}{14}{subsubsection.6.4.1}%
\contentsline {subsubsection}{\numberline {6.4.2}Calibrated Models}{14}{subsubsection.6.4.2}%
\contentsline {subsubsection}{\numberline {6.4.3}Balanced Random Forest Results}{14}{subsubsection.6.4.3}%
\contentsline {subsubsection}{\numberline {6.4.4}Results With Top 10 Features}{15}{subsubsection.6.4.4}%
\contentsline {subsubsection}{\numberline {6.4.5}Results With Top 20 Features}{15}{subsubsection.6.4.5}%
\contentsline {subsection}{\numberline {6.5}Classification Models}{15}{subsection.6.5}%
\contentsline {subsubsection}{\numberline {6.5.1}Results With Top 10 Features}{15}{subsubsection.6.5.1}%
\contentsline {subsection}{\numberline {6.6}One-Class Classification}{15}{subsection.6.6}%
\contentsline {section}{\numberline {7}Encoding Categorical Data}{16}{section.7}%
\contentsline {section}{\numberline {8}SHAP}{18}{section.8}%
\contentsline {subsection}{\numberline {8.1}Global SHAP Summary Values Results}{18}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Global SHAP Interaction Values}{21}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Multi-output Decision Plot for properly classified Rows}{22}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Decision Plot for Miss-classified Row}{24}{subsection.8.4}%
\contentsline {section}{\numberline {9}Chapter 6: Precision, Recall, and F-measure}{24}{section.9}%
\contentsline {section}{\numberline {10}Chapter 7:ROC Curves and Precision-Recall Curves}{24}{section.10}%
\contentsline {subsection}{\numberline {10.1}ROC Curve}{24}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}AUC}{25}{subsection.10.2}%
\contentsline {section}{\numberline {11}Chapter 8: Probability Scoring Methods}{25}{section.11}%
\contentsline {subsection}{\numberline {11.1}Probability Metrics}{25}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}LogLoss Score}{25}{subsection.11.2}%
\contentsline {section}{\numberline {12}Cross Validation for Imbalanced Datasets}{25}{section.12}%
\contentsline {section}{\numberline {13}Chapter 10}{25}{section.13}%
\contentsline {section}{\numberline {14}Chapter 12}{26}{section.14}%
\contentsline {subsection}{\numberline {14.1}Oversampling}{26}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Undersampling Techniques}{26}{subsection.14.2}%
\contentsline {section}{\numberline {15}Data Transforms (Hiyam)}{26}{section.15}%
\contentsline {subsection}{\numberline {15.1}Scaling Numeric Data (chapter 17)}{26}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Scaling Data with Outliers (chapter 18)}{26}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}How to Encode Categorical Data (chapter 19)}{27}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}How to Make Distributions Look More Gaussian (chapter 20)}{28}{subsection.15.4}%
\contentsline {section}{\numberline {16}Data Pre-Pocessing}{28}{section.16}%
\contentsline {subsection}{\numberline {16.1}Imputing Missing Values - Categorical Features}{28}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}Imputing Missing Values - Numerical/Ordinal Features}{28}{subsection.16.2}%
\contentsline {subsection}{\numberline {16.3}Scaling - Numeric/Ordinal Features}{29}{subsection.16.3}%
\contentsline {subsection}{\numberline {16.4}Scaling - Categorical Features}{29}{subsection.16.4}%
\contentsline {section}{\numberline {17}Imputing Numerical Missing Data}{29}{section.17}%
\contentsline {subsection}{\numberline {17.1}Statistical Imputation}{29}{subsection.17.1}%
\contentsline {subsection}{\numberline {17.2}K Nearest Neighbors}{29}{subsection.17.2}%
\contentsline {subsection}{\numberline {17.3}Iterative Imputation}{29}{subsection.17.3}%
\contentsline {section}{\numberline {18}Imputing Categorical Missing Data}{30}{section.18}%
\contentsline {subsection}{\numberline {18.1}Mean Imputation}{30}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Imputation Using Most Frequent or (Zero/Constant Values)}{30}{subsection.18.2}%
\contentsline {subsection}{\numberline {18.3}Create a New Category (Random Category) for NAN Values}{30}{subsection.18.3}%
\contentsline {subsection}{\numberline {18.4} Adding a Variable To Capture NAN}{30}{subsection.18.4}%
\contentsline {subsection}{\numberline {18.5}Imputation Using Deep Learning (Datawig)}{30}{subsection.18.5}%
\contentsline {subsection}{\numberline {18.6}Regression Imputation}{30}{subsection.18.6}%
\contentsline {subsection}{\numberline {18.7}Imputation Using Interpolation}{31}{subsection.18.7}%
\contentsline {subsection}{\numberline {18.8}Multivariate Normal Imputation (MNVI)}{31}{subsection.18.8}%
\contentsline {subsection}{\numberline {18.9}Multiple Imputation by Chained Equations (MICE)}{31}{subsection.18.9}%
\contentsline {subsection}{\numberline {18.10}Stochastic regression imputation:}{32}{subsection.18.10}%
\contentsline {subsection}{\numberline {18.11}Extrapolation and Interpolation:}{32}{subsection.18.11}%
\contentsline {subsection}{\numberline {18.12}Hot-Deck Imputation}{32}{subsection.18.12}%
\contentsline {subsection}{\numberline {18.13}Some papers for handling missing categorical data (did not read them at length -- just the abstract)}{32}{subsection.18.13}%
\contentsline {section}{\numberline {19}Advanced ML Evaluation Techniques}{32}{section.19}%
\contentsline {subsection}{\numberline {19.1}Analysis of Predictive Models}{33}{subsection.19.1}%
\contentsline {subsection}{\numberline {19.2}Evaluation Using Traditional Metrics}{33}{subsection.19.2}%
\contentsline {subsection}{\numberline {19.3}Problem with Standard Evaluation Metrics}{34}{subsection.19.3}%
\contentsline {subsection}{\numberline {19.4}Solution: Risk Estimates}{34}{subsection.19.4}%
\contentsline {subsection}{\numberline {19.5}Ensuring Quality Of Risk Estimates}{35}{subsection.19.5}%
\contentsline {subsubsection}{\numberline {19.5.1}From Models to Risk Estimates}{35}{subsubsection.19.5.1}%
\contentsline {subsubsection}{\numberline {19.5.2}Measuring the Goodness of Risk Scores}{35}{subsubsection.19.5.2}%
\contentsline {subsubsection}{\numberline {19.5.3}Comparative Evaluation of Risk Estimates}{36}{subsubsection.19.5.3}%
\contentsline {subsection}{\numberline {19.6}Interpreting Classifier Outputs - FP Growth}{38}{subsection.19.6}%
\contentsline {subsection}{\numberline {19.7}Technicalities}{38}{subsection.19.7}%
\contentsline {subsection}{\numberline {19.8}Characterizing Prediction Mistakes}{39}{subsection.19.8}%
\contentsline {subsection}{\numberline {19.9}Comparing Classifier Predictions}{39}{subsection.19.9}%
\contentsline {section}{\numberline {20}Plan for the Paper}{40}{section.20}%
\contentsline {section}{\numberline {21}Model Agnostic Meta Learning (MAML by Chelsea Finn)}{40}{section.21}%
\contentsline {subsection}{\numberline {21.1}Code Modifications}{41}{subsection.21.1}%
\contentsline {subsection}{\numberline {21.2}Modification of Data Generation Process}{42}{subsection.21.2}%
\contentsline {subsection}{\numberline {21.3}Incorporating FP Growth}{43}{subsection.21.3}%
\contentsline {subsection}{\numberline {21.4}Results}{45}{subsection.21.4}%
\contentsline {section}{\numberline {22}Sampling Training/Testing Tasks}{46}{section.22}%
\contentsline {section}{\numberline {23}Meta Learning Vs. Shallow Models - FAKES}{47}{section.23}%
\contentsline {subsection}{\numberline {23.1}Methodology}{47}{subsection.23.1}%
\contentsline {subsection}{\numberline {23.2}Frequent Patterns \& FP Growth}{48}{subsection.23.2}%
\contentsline {subsection}{\numberline {23.3}Hyperparameters}{50}{subsection.23.3}%
\contentsline {subsection}{\numberline {23.4}Winning Hyper Parameters}{51}{subsection.23.4}%
\contentsline {subsection}{\numberline {23.5}Quantitative Results}{54}{subsection.23.5}%
\contentsline {subsection}{\numberline {23.6}Qualitative Results}{56}{subsection.23.6}%
\contentsline {subsection}{\numberline {23.7}Mean Empirical Risk Curves}{56}{subsection.23.7}%
\contentsline {subsection}{\numberline {23.8}Precisions Top K}{57}{subsection.23.8}%
\contentsline {subsection}{\numberline {23.9}Recalls Top K}{58}{subsection.23.9}%
\contentsline {subsection}{\numberline {23.10}ROC Curves}{59}{subsection.23.10}%
\contentsline {subsection}{\numberline {23.11}Characterizing Prediction Mistakes}{59}{subsection.23.11}%
\contentsline {subsection}{\numberline {23.12}Conclusion}{61}{subsection.23.12}%
\contentsline {section}{\numberline {24}Meta Learning vs Shallow Models - Dementia Dataset}{61}{section.24}%
\contentsline {subsection}{\numberline {24.1}Cost Sensitive Learning in Neural Networks}{61}{subsection.24.1}%
\contentsline {subsection}{\numberline {24.2}Hyper Parameters}{63}{subsection.24.2}%
\contentsline {subsection}{\numberline {24.3}Winning Hyper Parameters}{63}{subsection.24.3}%
\contentsline {subsection}{\numberline {24.4}Quantitative Results - Without FP Growth}{66}{subsection.24.4}%
\contentsline {subsection}{\numberline {24.5}Insights}{68}{subsection.24.5}%
\contentsline {subsection}{\numberline {24.6}Quantitative Results - With FP Growth}{69}{subsection.24.6}%
\contentsline {subsection}{\numberline {24.7}Insights}{71}{subsection.24.7}%
\contentsline {subsection}{\numberline {24.8}Insights on With FP vs. Without FP}{71}{subsection.24.8}%
\contentsline {subsection}{\numberline {24.9}Insights about PPV}{71}{subsection.24.9}%
\contentsline {subsection}{\numberline {24.10}Qualitative Results}{72}{subsection.24.10}%
\contentsline {subsubsection}{\numberline {24.10.1}Mean Empirical Risk Curves}{72}{subsubsection.24.10.1}%
\contentsline {subsubsection}{\numberline {24.10.2}Precision at Top K}{73}{subsubsection.24.10.2}%
\contentsline {subsubsection}{\numberline {24.10.3}Recall at Top K}{74}{subsubsection.24.10.3}%
\contentsline {subsubsection}{\numberline {24.10.4}ROC Curves}{75}{subsubsection.24.10.4}%
