\contentsline {section}{\numberline {1}Data Sources}{4}{section.1}%
\contentsline {subsection}{\numberline {1.1}Datasets Used}{4}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Dementia Output}{5}{subsection.1.2}%
\contentsline {section}{\numberline {2}Split Into numeric and textual}{5}{section.2}%
\contentsline {section}{\numberline {3}Numeric Features}{5}{section.3}%
\contentsline {subsection}{\numberline {3.1}Informant and High Percentage of Missing}{6}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Nested Questions and High percentage of missing}{7}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Treating Features with High Percentage of Missing}{7}{subsection.3.3}%
\contentsline {subsection}{\numberline {3.4}Detecting Erroneous Values Inside Features}{8}{subsection.3.4}%
\contentsline {subsection}{\numberline {3.5}Working with Legal Features}{8}{subsection.3.5}%
\contentsline {subsection}{\numberline {3.6}Filtering Legal Features with Erroneous Values - Erroneous Code-Book}{8}{subsection.3.6}%
\contentsline {subsection}{\numberline {3.7}Detecting Outliers}{9}{subsection.3.7}%
\contentsline {subsection}{\numberline {3.8}Outlier Detection and Scaling}{10}{subsection.3.8}%
\contentsline {subsection}{\numberline {3.9}Erroneous Codebook}{10}{subsection.3.9}%
\contentsline {subsection}{\numberline {3.10}Treating Erroneous Values}{11}{subsection.3.10}%
\contentsline {subsection}{\numberline {3.11}Data Preprocessing}{11}{subsection.3.11}%
\contentsline {subsubsection}{\numberline {3.11.1}Imputing Missing Values}{11}{subsubsection.3.11.1}%
\contentsline {subsubsection}{\numberline {3.11.2}Scaling Features}{11}{subsubsection.3.11.2}%
\contentsline {section}{\numberline {4}Feature Selection}{11}{section.4}%
\contentsline {subsection}{\numberline {4.1}Decision Tree Feature Importance}{11}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Select K Best Features}{12}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Deciding "K"}{12}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}Selecting K Best Features}{12}{subsubsection.4.2.2}%
\contentsline {section}{\numberline {5}Oversampling and Undersampling}{12}{section.5}%
\contentsline {subsection}{\numberline {5.1}Dealing With Mixed Variables}{12}{subsection.5.1}%
\contentsline {section}{\numberline {6}Cost-Sensitive Learning}{13}{section.6}%
\contentsline {subsection}{\numberline {6.1}Cost sensitive learning consists of the following steps:}{13}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Tested sampling methods (using SMOTENC's sampling\_strategy options):}{13}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Tested class\_weights: (costs for each label), combination of suggestions from book and built-in options:}{14}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Probabilistic Models}{14}{subsection.6.4}%
\contentsline {subsubsection}{\numberline {6.4.1}Grid Search For Optimal Probability Threshold}{14}{subsubsection.6.4.1}%
\contentsline {subsubsection}{\numberline {6.4.2}Calibrated Models}{14}{subsubsection.6.4.2}%
\contentsline {subsubsection}{\numberline {6.4.3}Balanced Random Forest Results}{14}{subsubsection.6.4.3}%
\contentsline {subsubsection}{\numberline {6.4.4}Results}{15}{subsubsection.6.4.4}%
\contentsline {subsubsection}{\numberline {6.4.5}Results With Top 10 Features}{15}{subsubsection.6.4.5}%
\contentsline {subsubsection}{\numberline {6.4.6}Results With Top 20 Features}{16}{subsubsection.6.4.6}%
\contentsline {subsection}{\numberline {6.5}Classfication Models}{16}{subsection.6.5}%
\contentsline {subsubsection}{\numberline {6.5.1}Results}{16}{subsubsection.6.5.1}%
\contentsline {subsubsection}{\numberline {6.5.2}Results With Top 10 Features}{16}{subsubsection.6.5.2}%
\contentsline {subsubsection}{\numberline {6.5.3}Results With Top 20 Features}{16}{subsubsection.6.5.3}%
\contentsline {subsection}{\numberline {6.6}One-Class Classification}{16}{subsection.6.6}%
\contentsline {section}{\numberline {7}Encoding Categorical Data}{17}{section.7}%
\contentsline {section}{\numberline {8}SHAP}{19}{section.8}%
\contentsline {subsection}{\numberline {8.1}Global SHAP Summary Values Results}{19}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Global SHAP Interaction Values}{22}{subsection.8.2}%
\contentsline {subsection}{\numberline {8.3}Multi-output Decision Plot for properly classified Rows}{23}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Decision Plot for Miss-classified Row}{25}{subsection.8.4}%
\contentsline {section}{\numberline {9}Chapter 6: Precision, Recall, and F-measure}{25}{section.9}%
\contentsline {section}{\numberline {10}Chapter 7:ROC Curves and Precision-Recall Curves}{25}{section.10}%
\contentsline {subsection}{\numberline {10.1}ROC Curve}{25}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}AUC}{26}{subsection.10.2}%
\contentsline {section}{\numberline {11}Chapter 8: Probability Scoring Methods}{26}{section.11}%
\contentsline {subsection}{\numberline {11.1}Probability Metrics}{26}{subsection.11.1}%
\contentsline {subsection}{\numberline {11.2}LogLoss Score}{26}{subsection.11.2}%
\contentsline {section}{\numberline {12}Cross Validation for Imbalanced Datasets}{26}{section.12}%
\contentsline {section}{\numberline {13}Chapter 10}{26}{section.13}%
\contentsline {section}{\numberline {14}Chapter 12}{27}{section.14}%
\contentsline {subsection}{\numberline {14.1}Oversampling}{27}{subsection.14.1}%
\contentsline {subsection}{\numberline {14.2}Undersampling Techniques}{27}{subsection.14.2}%
\contentsline {section}{\numberline {15}Data Transforms (Hiyam)}{27}{section.15}%
\contentsline {subsection}{\numberline {15.1}Scaling Numeric Data (chapter 17)}{27}{subsection.15.1}%
\contentsline {subsection}{\numberline {15.2}Scaling Data with Outliers (chapter 18)}{27}{subsection.15.2}%
\contentsline {subsection}{\numberline {15.3}How to Encode Categorical Data (chapter 19)}{28}{subsection.15.3}%
\contentsline {subsection}{\numberline {15.4}How to Make Distributions Look More Gaussian (chapter 20)}{29}{subsection.15.4}%
\contentsline {section}{\numberline {16}Data Pre-Pocessing}{29}{section.16}%
\contentsline {subsection}{\numberline {16.1}Imputing Missing Values - Categorical Features}{29}{subsection.16.1}%
\contentsline {subsection}{\numberline {16.2}Imputing Missing Values - Numerical/Ordinal Features}{29}{subsection.16.2}%
\contentsline {subsection}{\numberline {16.3}Scaling - Numeric/Ordinal Features}{30}{subsection.16.3}%
\contentsline {subsection}{\numberline {16.4}Scaling - Categorical Features}{30}{subsection.16.4}%
\contentsline {section}{\numberline {17}Imputing Numerical Missing Data}{30}{section.17}%
\contentsline {subsection}{\numberline {17.1}Statistical Imputation}{30}{subsection.17.1}%
\contentsline {subsection}{\numberline {17.2}K Nearest Neighbors}{30}{subsection.17.2}%
\contentsline {subsection}{\numberline {17.3}Iterative Imputation}{30}{subsection.17.3}%
\contentsline {section}{\numberline {18}Imputing Categorical Missing Data}{31}{section.18}%
\contentsline {subsection}{\numberline {18.1}Mean Imputation}{31}{subsection.18.1}%
\contentsline {subsection}{\numberline {18.2}Imputation Using Most Frequent or (Zero/Constant Values)}{31}{subsection.18.2}%
\contentsline {subsection}{\numberline {18.3}Create a New Category (Random Category) for NAN Values}{31}{subsection.18.3}%
\contentsline {subsection}{\numberline {18.4} Adding a Variable To Capture NAN}{31}{subsection.18.4}%
\contentsline {subsection}{\numberline {18.5}Imputation Using Deep Learning (Datawig)}{31}{subsection.18.5}%
\contentsline {subsection}{\numberline {18.6}Regression Imputation}{31}{subsection.18.6}%
\contentsline {subsection}{\numberline {18.7}Imputation Using Interpolation}{32}{subsection.18.7}%
\contentsline {subsection}{\numberline {18.8}Multivariate Normal Imputation (MNVI)}{32}{subsection.18.8}%
\contentsline {subsection}{\numberline {18.9}Multiple Imputation by Chained Equations (MICE)}{32}{subsection.18.9}%
\contentsline {subsection}{\numberline {18.10}Stochastic regression imputation:}{33}{subsection.18.10}%
\contentsline {subsection}{\numberline {18.11}Extrapolation and Interpolation:}{33}{subsection.18.11}%
\contentsline {subsection}{\numberline {18.12}Hot-Deck Imputation}{33}{subsection.18.12}%
\contentsline {subsection}{\numberline {18.13}Some papers for handling missing categorical data (did not read them at length -- just the abstract)}{33}{subsection.18.13}%
\contentsline {section}{\numberline {19}Advanced ML Evaluation Techniques}{33}{section.19}%
\contentsline {subsection}{\numberline {19.1}Analysis of Predictive Models}{34}{subsection.19.1}%
\contentsline {subsection}{\numberline {19.2}Evaluation Using Traditional Metrics}{34}{subsection.19.2}%
\contentsline {subsection}{\numberline {19.3}Problem with Standard Evaluation Metrics}{35}{subsection.19.3}%
\contentsline {subsection}{\numberline {19.4}Solution: Risk Estimates}{35}{subsection.19.4}%
\contentsline {subsection}{\numberline {19.5}Ensuring Quality Of Risk Estimates}{36}{subsection.19.5}%
\contentsline {subsubsection}{\numberline {19.5.1}From Models to Risk Estimates}{36}{subsubsection.19.5.1}%
\contentsline {subsubsection}{\numberline {19.5.2}Measuring the Goodness of Risk Scores}{36}{subsubsection.19.5.2}%
\contentsline {subsubsection}{\numberline {19.5.3}Comparative Evaluation of Risk Estimates}{37}{subsubsection.19.5.3}%
\contentsline {subsection}{\numberline {19.6}Interpreting Classifier Outputs - FP Growth}{39}{subsection.19.6}%
\contentsline {subsection}{\numberline {19.7}Technicalities}{39}{subsection.19.7}%
\contentsline {subsection}{\numberline {19.8}Characterizing Prediction Mistakes}{40}{subsection.19.8}%
\contentsline {subsection}{\numberline {19.9}Comparing Classifier Predictions}{40}{subsection.19.9}%
\contentsline {section}{\numberline {20}Model Agnostic Meta Learning (MAML by Chelsea Finn)}{41}{section.20}%
\contentsline {subsection}{\numberline {20.1}Code Modifications}{41}{subsection.20.1}%
\contentsline {subsection}{\numberline {20.2}Modification of Data Generation Process}{43}{subsection.20.2}%
\contentsline {subsection}{\numberline {20.3}Incorporating FP Growth}{44}{subsection.20.3}%
\contentsline {subsection}{\numberline {20.4}Results}{46}{subsection.20.4}%
\contentsline {section}{\numberline {21}Sampling Training/Testing Tasks}{47}{section.21}%
\contentsline {section}{\numberline {22}Meta Learning Vs. Shallow Models - FAKES}{48}{section.22}%
\contentsline {subsection}{\numberline {22.1}Methodology}{48}{subsection.22.1}%
\contentsline {subsection}{\numberline {22.2}Frequent Patterns \& FP Growth}{49}{subsection.22.2}%
\contentsline {subsection}{\numberline {22.3}Hyperparameters}{51}{subsection.22.3}%
\contentsline {subsection}{\numberline {22.4}Winning Hyper Parameters}{52}{subsection.22.4}%
\contentsline {subsection}{\numberline {22.5}Quantitative Results}{55}{subsection.22.5}%
\contentsline {subsection}{\numberline {22.6}Qualitative Results}{57}{subsection.22.6}%
\contentsline {subsection}{\numberline {22.7}Mean Empirical Risk Curves}{57}{subsection.22.7}%
\contentsline {subsection}{\numberline {22.8}Precisions Top K}{58}{subsection.22.8}%
\contentsline {subsection}{\numberline {22.9}Recalls Top K}{59}{subsection.22.9}%
\contentsline {subsection}{\numberline {22.10}ROC Curves}{60}{subsection.22.10}%
\contentsline {subsection}{\numberline {22.11}Characterizing Prediction Mistakes}{60}{subsection.22.11}%
\contentsline {subsection}{\numberline {22.12}Conclusion}{62}{subsection.22.12}%
\contentsline {section}{\numberline {23}Meta Learning vs Shallow Models - Dementia Dataset}{62}{section.23}%
\contentsline {subsection}{\numberline {23.1}Cost Sensitive Learning in Neural Networks}{62}{subsection.23.1}%
\contentsline {subsection}{\numberline {23.2}Hyper Parameters}{64}{subsection.23.2}%
\contentsline {subsection}{\numberline {23.3}Winning Hyper Parameters}{65}{subsection.23.3}%
\contentsline {subsection}{\numberline {23.4}Quantitative Results - Without FP Growth}{66}{subsection.23.4}%
\contentsline {subsection}{\numberline {23.5}Insights}{68}{subsection.23.5}%
\contentsline {subsection}{\numberline {23.6}Quantitative Results - With FP Growth}{68}{subsection.23.6}%
\contentsline {subsection}{\numberline {23.7}Insights}{70}{subsection.23.7}%
\contentsline {subsection}{\numberline {23.8}Insights on With FP vs. Without FP}{70}{subsection.23.8}%
\contentsline {subsection}{\numberline {23.9}Insights about PPV}{71}{subsection.23.9}%
\contentsline {subsection}{\numberline {23.10}Qualitative Results}{71}{subsection.23.10}%
\contentsline {subsection}{\numberline {23.11}Mean Empirical Risk Curves}{71}{subsection.23.11}%
\contentsline {subsection}{\numberline {23.12}Precision at Top K}{73}{subsection.23.12}%
\contentsline {subsection}{\numberline {23.13}Recall at Top K}{74}{subsection.23.13}%
\contentsline {subsection}{\numberline {23.14}ROC Curves}{75}{subsection.23.14}%
