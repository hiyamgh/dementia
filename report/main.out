\BOOKMARK [1][-]{section.1}{Data Sources}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Datasets Used}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Dementia Output}{section.1}% 3
\BOOKMARK [1][-]{section.2}{Split Into numeric and textual}{}% 4
\BOOKMARK [1][-]{section.3}{Numeric Features}{}% 5
\BOOKMARK [2][-]{subsection.3.1}{Informant and High Percentage of Missing}{section.3}% 6
\BOOKMARK [2][-]{subsection.3.2}{Nested Questions and High percentage of missing}{section.3}% 7
\BOOKMARK [2][-]{subsection.3.3}{Treating Features with High Percentage of Missing}{section.3}% 8
\BOOKMARK [2][-]{subsection.3.4}{Detecting Erroneous Values Inside Features}{section.3}% 9
\BOOKMARK [2][-]{subsection.3.5}{Working with Legal Features}{section.3}% 10
\BOOKMARK [2][-]{subsection.3.6}{Filtering Legal Features with Erroneous Values - Erroneous Code-Book}{section.3}% 11
\BOOKMARK [2][-]{subsection.3.7}{Detecting Outliers}{section.3}% 12
\BOOKMARK [2][-]{subsection.3.8}{Outlier Detection and Scaling}{section.3}% 13
\BOOKMARK [2][-]{subsection.3.9}{Erroneous Codebook}{section.3}% 14
\BOOKMARK [2][-]{subsection.3.10}{Treating Erroneous Values}{section.3}% 15
\BOOKMARK [2][-]{subsection.3.11}{Data Preprocessing}{section.3}% 16
\BOOKMARK [3][-]{subsubsection.3.11.1}{Imputing Missing Values}{subsection.3.11}% 17
\BOOKMARK [3][-]{subsubsection.3.11.2}{Scaling Features}{subsection.3.11}% 18
\BOOKMARK [1][-]{section.4}{Feature Selection}{}% 19
\BOOKMARK [2][-]{subsection.4.1}{Decision Tree Feature Importance}{section.4}% 20
\BOOKMARK [2][-]{subsection.4.2}{Select K Best Features}{section.4}% 21
\BOOKMARK [3][-]{subsubsection.4.2.1}{Deciding "K"}{subsection.4.2}% 22
\BOOKMARK [3][-]{subsubsection.4.2.2}{Selecting K Best Features}{subsection.4.2}% 23
\BOOKMARK [1][-]{section.5}{Oversampling and Undersampling}{}% 24
\BOOKMARK [2][-]{subsection.5.1}{Dealing With Mixed Variables}{section.5}% 25
\BOOKMARK [1][-]{section.6}{Cost-Sensitive Learning}{}% 26
\BOOKMARK [2][-]{subsection.6.1}{Cost sensitive learning consists of the following steps:}{section.6}% 27
\BOOKMARK [2][-]{subsection.6.2}{Tested sampling methods \(using SMOTENC's sampling\137strategy options\):}{section.6}% 28
\BOOKMARK [2][-]{subsection.6.3}{Tested class\137weights: \(costs for each label\), combination of suggestions from book and built-in options:}{section.6}% 29
\BOOKMARK [2][-]{subsection.6.4}{Probabilistic Models}{section.6}% 30
\BOOKMARK [3][-]{subsubsection.6.4.1}{Grid Search For Optimal Probability Threshold}{subsection.6.4}% 31
\BOOKMARK [3][-]{subsubsection.6.4.2}{Calibrated Models}{subsection.6.4}% 32
\BOOKMARK [3][-]{subsubsection.6.4.3}{Balanced Random Forest Results}{subsection.6.4}% 33
\BOOKMARK [2][-]{subsection.6.5}{Results - Probabilistic}{section.6}% 34
\BOOKMARK [2][-]{subsection.6.6}{Results - Classification}{section.6}% 35
\BOOKMARK [2][-]{subsection.6.7}{Results - One Class Classification}{section.6}% 36
\BOOKMARK [1][-]{section.7}{Encoding Categorical Data}{}% 37
\BOOKMARK [1][-]{section.8}{SHAP}{}% 38
\BOOKMARK [2][-]{subsection.8.1}{Global SHAP Summary Values Results}{section.8}% 39
\BOOKMARK [2][-]{subsection.8.2}{Global SHAP Interaction Values}{section.8}% 40
\BOOKMARK [2][-]{subsection.8.3}{Multi-output Decision Plot for properly classified Rows}{section.8}% 41
\BOOKMARK [2][-]{subsection.8.4}{Decision Plot for Miss-classified Row}{section.8}% 42
\BOOKMARK [1][-]{section.9}{Chapter 6: Precision, Recall, and F-measure}{}% 43
\BOOKMARK [1][-]{section.10}{Chapter 7:ROC Curves and Precision-Recall Curves}{}% 44
\BOOKMARK [2][-]{subsection.10.1}{ROC Curve}{section.10}% 45
\BOOKMARK [2][-]{subsection.10.2}{AUC}{section.10}% 46
\BOOKMARK [1][-]{section.11}{Chapter 8: Probability Scoring Methods}{}% 47
\BOOKMARK [2][-]{subsection.11.1}{Probability Metrics}{section.11}% 48
\BOOKMARK [2][-]{subsection.11.2}{LogLoss Score}{section.11}% 49
\BOOKMARK [1][-]{section.12}{Cross Validation for Imbalanced Datasets}{}% 50
\BOOKMARK [1][-]{section.13}{Chapter 10}{}% 51
\BOOKMARK [1][-]{section.14}{Chapter 12}{}% 52
\BOOKMARK [2][-]{subsection.14.1}{Oversampling}{section.14}% 53
\BOOKMARK [2][-]{subsection.14.2}{Undersampling Techniques}{section.14}% 54
\BOOKMARK [1][-]{section.15}{Data Transforms \(Hiyam\)}{}% 55
\BOOKMARK [2][-]{subsection.15.1}{Scaling Numeric Data \(chapter 17\)}{section.15}% 56
\BOOKMARK [2][-]{subsection.15.2}{Scaling Data with Outliers \(chapter 18\)}{section.15}% 57
\BOOKMARK [2][-]{subsection.15.3}{How to Encode Categorical Data \(chapter 19\)}{section.15}% 58
\BOOKMARK [2][-]{subsection.15.4}{How to Make Distributions Look More Gaussian \(chapter 20\)}{section.15}% 59
\BOOKMARK [1][-]{section.16}{Data Pre-Pocessing}{}% 60
\BOOKMARK [2][-]{subsection.16.1}{Imputing Missing Values - Categorical Features}{section.16}% 61
\BOOKMARK [2][-]{subsection.16.2}{Imputing Missing Values - Numerical/Ordinal Features}{section.16}% 62
\BOOKMARK [2][-]{subsection.16.3}{Scaling - Numeric/Ordinal Features}{section.16}% 63
\BOOKMARK [2][-]{subsection.16.4}{Scaling - Categorical Features}{section.16}% 64
\BOOKMARK [1][-]{section.17}{Imputing Numerical Missing Data}{}% 65
\BOOKMARK [2][-]{subsection.17.1}{Statistical Imputation}{section.17}% 66
\BOOKMARK [2][-]{subsection.17.2}{K Nearest Neighbors}{section.17}% 67
\BOOKMARK [2][-]{subsection.17.3}{Iterative Imputation}{section.17}% 68
\BOOKMARK [1][-]{section.18}{Imputing Categorical Missing Data}{}% 69
\BOOKMARK [2][-]{subsection.18.1}{Mean Imputation}{section.18}% 70
\BOOKMARK [2][-]{subsection.18.2}{Imputation Using Most Frequent or \(Zero/Constant Values\)}{section.18}% 71
\BOOKMARK [2][-]{subsection.18.3}{Create a New Category \(Random Category\) for NAN Values}{section.18}% 72
\BOOKMARK [2][-]{subsection.18.4}{ Adding a Variable To Capture NAN}{section.18}% 73
\BOOKMARK [2][-]{subsection.18.5}{Imputation Using Deep Learning \(Datawig\)}{section.18}% 74
\BOOKMARK [2][-]{subsection.18.6}{Regression Imputation}{section.18}% 75
\BOOKMARK [2][-]{subsection.18.7}{Imputation Using Interpolation}{section.18}% 76
\BOOKMARK [2][-]{subsection.18.8}{Multivariate Normal Imputation \(MNVI\)}{section.18}% 77
\BOOKMARK [2][-]{subsection.18.9}{Multiple Imputation by Chained Equations \(MICE\)}{section.18}% 78
\BOOKMARK [2][-]{subsection.18.10}{Stochastic regression imputation:}{section.18}% 79
\BOOKMARK [2][-]{subsection.18.11}{Extrapolation and Interpolation:}{section.18}% 80
\BOOKMARK [2][-]{subsection.18.12}{Hot-Deck Imputation}{section.18}% 81
\BOOKMARK [2][-]{subsection.18.13}{Some papers for handling missing categorical data \(did not read them at length \205 just the abstract\)}{section.18}% 82
\BOOKMARK [1][-]{section.19}{Plan for the Paper}{}% 83
\BOOKMARK [1][-]{section.20}{MAML - Sampling Training/Testing Tasks}{}% 84
\BOOKMARK [1][-]{section.21}{MAML vs Shallow Models - Dementia Dataset}{}% 85
\BOOKMARK [2][-]{subsection.21.1}{Cost Sensitive Learning in Neural Networks}{section.21}% 86
\BOOKMARK [2][-]{subsection.21.2}{Hyper Parameters}{section.21}% 87
\BOOKMARK [2][-]{subsection.21.3}{Winning Hyper Parameters}{section.21}% 88
\BOOKMARK [2][-]{subsection.21.4}{Quantitative Results - Without FP Growth}{section.21}% 89
\BOOKMARK [2][-]{subsection.21.5}{Insights}{section.21}% 90
\BOOKMARK [2][-]{subsection.21.6}{Quantitative Results - With FP Growth}{section.21}% 91
\BOOKMARK [2][-]{subsection.21.7}{Insights}{section.21}% 92
\BOOKMARK [2][-]{subsection.21.8}{Insights on With FP vs. Without FP}{section.21}% 93
\BOOKMARK [2][-]{subsection.21.9}{Insights about PPV}{section.21}% 94
\BOOKMARK [2][-]{subsection.21.10}{Qualitative Results}{section.21}% 95
\BOOKMARK [3][-]{subsubsection.21.10.1}{Mean Empirical Risk Curves}{subsection.21.10}% 96
\BOOKMARK [3][-]{subsubsection.21.10.2}{Precision at Top K}{subsection.21.10}% 97
\BOOKMARK [3][-]{subsubsection.21.10.3}{Recall at Top K}{subsection.21.10}% 98
\BOOKMARK [3][-]{subsubsection.21.10.4}{ROC Curves}{subsection.21.10}% 99
\BOOKMARK [1][-]{section.22}{ARML - Dementia}{}% 100
\BOOKMARK [2][-]{subsection.22.1}{Quantitative Results - Dementia Top 10 Features}{section.22}% 101
\BOOKMARK [2][-]{subsection.22.2}{Quantitative Results - Dementia Top 20 Features}{section.22}% 102
\BOOKMARK [2][-]{subsection.22.3}{Winning Hyper Parameters}{section.22}% 103
\BOOKMARK [2][-]{subsection.22.4}{Qualitative Results}{section.22}% 104
\BOOKMARK [3][-]{subsubsection.22.4.1}{Mean Empirical Risk Curves}{subsection.22.4}% 105
\BOOKMARK [3][-]{subsubsection.22.4.2}{Precision at Top K}{subsection.22.4}% 106
\BOOKMARK [3][-]{subsubsection.22.4.3}{Recall at Top K}{subsection.22.4}% 107
\BOOKMARK [3][-]{subsubsection.22.4.4}{ROC Curves}{subsection.22.4}% 108
\BOOKMARK [2][-]{subsection.22.5}{Frequent Patterns and Probability of Mistakes}{section.22}% 109
\BOOKMARK [1][-]{section.23}{All Results - Dementia - Quantitative}{}% 110
\BOOKMARK [1][-]{section.24}{Reptile}{}% 111
\BOOKMARK [2][-]{subsection.24.1}{Hyper Parameters}{section.24}% 112
