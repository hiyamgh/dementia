\documentclass{article}
\usepackage[utf8]{inputenc}
\PassOptionsToPackage{hyphens}{url}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[section]{placeins}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{multirow}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage[T1]{fontenc}
\usepackage[normalem]{ulem}
\usepackage{footmisc}
\useunder{\uline}{\ul}{}

\title{Dementia}

\begin{document}

\maketitle

\tableofcontents

\section{Data Sources}
\subsection{Datasets Used}
The datasets used are from the following dementia surveys conducted in Lebanon:
\begin{itemize}
    \item Bekaa Questionnaire: This dataset is the most recent survey conducted last year in the Bekaa. This dataset involved 219 elderly.
    \item full validation data: This data consists of 281 rows. This is the very first survey that we implemented for the sake of testing the 1066 Dementia SPSS algorithm. This survey was conducted on a 1-1 ratio with clinically diagnosed demented patients.
    \item dementia data 502 baseline updated: This is the second survey implemented which involved 502 elderly people. The selection of elderly to participate in this survey was random.
\end{itemize}
We then merged this dataset into one dataset consisting of a total of 1024 elderly.

\subsection{Dementia Output}
In order to label each row (elderly) as positive (has dementia) or negative (does not have dementia), we run the 1066 Dementia SPSS algorithm on our dataset. The result was a total of 1024 patients, 203 of whom have dementia according to the SPSS algorithm.

\section{Split Into numeric and textual}
We have split the data into two chunks, one containing numeric features and another containing categorical features. The Bekaa and full validation questionnaires involved textual questions as well. However, these textual questions are not used by the dementia 1066 SPSS algorithm in determining whether the elderly has dementia or not, so we dropped these textual features from the analysis.

\section{Numeric Features}
\label{sec:numeric}
Our data is now a mix of numerical and categorical features. For the numeric features, we have obtained the following information:
\begin{itemize}
\item \textbf{data type:} Type of the numeric feature. Could possible be: 
\begin{enumerate}
\item \textbf{numeric} meaning this feature has continuous range of values

\item \textbf{categorical} meaning this feature has only a finite set of values, each resembling a specific category

\item \textbf{ordinal} meaning, this feature's values are categorical, but they can be compared between each other. Example would be: a job, were a specific job might be values more than the other. Example: manager vs assistant
\end{enumerate}

\item[] the specification of the \textbf{data type} of each feature was done manually by looking at the range of values in each, and also by looking at the \textcolor{blue}{choices workbook in the copy of dementia Excel questionnaire}

\item \textbf{Description:} Description of each feature

\item \textbf{cat\_options:} Short for "categorical options". \textit{In case the feature is categorical}, what are the possible categories.

\item \textbf{val\_range:} Short for "values range". \textit{In case the feature is numerical}, what are the range of values this feature takes

\item \textbf{min:} \textit{In case the feature is numeric}, what is the minimum value it has

\item \textbf{max:} \textit{In case the feature is numeric}, what is the maximum value it has

\item \textbf{distribution:} link to the distribution plot of each feature on bitbucket. The distribution plot helps in detecting erroneous data

\item \textbf{perc\_missing:} Short for "percentage of missing values". This helps us ggregate features with high percentage of missing. In case the percentage of missing is very high, Imputation methods won't aid much

\item \textbf{perc\_erroneous:} Short for "percentage of erroneous values". Detecting erroneous values was done manually for each feature. Anomalies cover uni-intentional or intentional erroneous values. Example would be having "age" feature being 1966 instead of the actual age

\item \textbf{erroneous:} What were the erroneous values, if any

\item \textbf{cut\_off:} The cut off used to decide whether a feature's value is erroneous or not. The cut off was discovered manually by us for each feature.

\item \textbf{color code:} a color code was applied to each feature to determine whether it belongs to the informant or not. The features having the color code \textcolor{yellow}{yellow} are \textbf{informant}, the ones having the color code \textcolor{red}{red} are related to the \textbf{patient} himself/herself. \footnote{The dataset containing information about all numeric features can be found on our public repository on github: \url{https://github.com/hiyamgh/dementia/blob/master/input/codebooks/numeric.xlsx} \label{numeric_codebook}}
\end{itemize} 


\subsection{Informant and High Percentage of Missing}
Most of the features associated with very high percentage of missing were directed to informants and related to the living conditions of the family, the house, income, cor-residents of the elderly, etc... which are not used by the SPSS algorithm to label the patients as demented or not (since they are not about the patients) \textbf{For this reason, we dropped the questions that were about the household, informants, and co-residents}

* informant is usually the caregiver of the interviewed elderly person

\subsection{Nested Questions and High percentage of missing}
Since our data is survey in nature, some features are actually questions are related to the other. If a certain question is asked to the elderly, and only based upon the elderly's answer, the interviewer might ask the elderly another question or not. Let us give an example:
\begin{enumerate}
\item the interviewer asks if there has ever been a period when the elderly smoked cigarettes, cigars, a pipe, or water pipe nearly every day?

\item \textbf{Only if the answer to the question above was a Yes}

\item the interviewer asks the following questions:
\begin{enumerate}
\item What did the elderly smoke?
\item How old were you when you started using tobacco regularly? (if the above is true)
\item Do you still use tobacco regularly?
\end{enumerate}
\end{enumerate}

We also realized that many features have high percentage of missing because a parent question's answer did not cause a jump to that feature question.


\subsection{Treating Features with High Percentage of Missing}
\label{sec:legalizingfeatures}
We divided the features as follows:
\begin{enumerate}
\item \textbf{Informant} A feature belongs to this category if the question is asked to the informant

\item From Informants we have:
\begin{enumerate}
\item \textbf{parent} Questions that caused jumps to other questions

\item \textbf{child} child of parent question
\end{enumerate}

\item \textbf{Non-Informant}
We also have:
\begin{enumerate}
\item \textbf{parent}
\item \textbf{child}
\end{enumerate}
\end{enumerate}

We followed the following steps in order to eliminate features with high percentage of missing:
\begin{enumerate}
\item remove all informants and their children questions (so we removed all informants, and if one of them happens to be a parent question, we removed all its children questions)

\item It is important to note that all parent questions informants have their children questions also informant

\item Then we are left with \textbf{Legal features}

\item From legal features, remove all children
\end{enumerate} 

\textbf{\textit{After doing the steps above, the number of \textcolor{blue}{numerical features} decreased from 542 to 257}}

\subsection{Detecting Erroneous Values Inside Features}
\label{sec:markerrneous}
In our dataset, the categorical and ordinal values are encoded with numbers ranging from 1 to the length of the categories. therefore, since our categorical, ordinal, and numerical values are all numbers, we set cut-offs to determine erroneous values. 
For some of the features, the erroneous values are straight forward to detect, based on the description for the question. These are the following:
\begin{itemize}
    \item \textbf{age}: some people enter the year, others enter the age. for this, if the value of both the numb as is or 2020 - the numb is more than 100, this means that the person is more than 100 years old which renders the age erroneous. For age, the cut-off was 100
    \item \textbf{helphour}: number of hours per week the elderly needed help, therefore bounded by max numb of hours in a week
    \item \textbf{learn questions}: the patient is supposed to repeat 3 words, bounded by the max numb of words: 3
\end{itemize}
For the rest of the categorical and ordinal values, we get the maximum number encoding for the question options from the options excel sheet based on which the interviewers selected these options. The cut-off is therefore the maximum number encoding a category. Were therefore label any value that is greater than the cut-off to be erroneous.

\subsection{Working with Legal Features}
As mentioned in section \ref{sec:legalizingfeatures}, the remaining \textbf{\textit{"Legal"}} Features are the 257 out of originally 542. We will be using these 257 \textbf{\textit{"Legal"}} features as input for data pre-processing and cleaning techniques.

\subsection{Filtering Legal Features with Erroneous Values - Erroneous Code-Book}
In section \ref{sec:numeric}, we talked about how some of the 542 numeric features have erroneous values. However, as we will be working with only "Legal" features, its important to filter out which subset of the features with erroneous values are actually "Legal". Thats's why, we did the following:

\begin{enumerate}
    \item From all the features containing erroneous, filter out the ones that are Legal
    \item From the Legal ones:
    \begin{enumerate}
        \item get the feature name
        \item get the feature's description
        \item get the feature's percentage of erroneous values
        \item get the feature's actual erroneous values
        \item Display the feature's cut-off on values which decides which values are erroneous (crossing the cut-off) and which are non-erroneous (not crossing the cut-off)
    \end{enumerate}
    \item When done from 2. above, sort the features by decreasing order of percentage of erroneous values
\end{enumerate}

\subsection{Detecting Outliers}
We want to investifgate the existsence of outliers in the data. We have 3 different data types:
\begin{enumerate}
    \item \textbf{Numeric}: very few columns
    \item \textbf{Ordinal}: categorical values that obey a certain order of importance
    \item \textbf{Categorical}
\end{enumerate}

\noindent The outlier detection methods are known for numeric values, but it is \textbf{ordianl} and \textbf{categorical} values that raise the question of how are we going to detect outliers with such types of data ?

\noindent We will assume that ordinal values are numeric. We will prove the legit-ability of our assumption through an example:

\noindent Assume for instance, that we have a column that shows the socio economic status (“low income”,”middle income”,”high income”), education level (“high school”,”BS”,”MS”,”PhD”), income level (“less than 50K”, “50K-100K”, “over 100K”), satisfaction rating (“extremely dislike”, “dislike”, “neutral”, “like”, “extremely like”). These are not categorical but rather ordinal, and if we give, for example, the income level the following labels:
\begin{itemize}
    \item less than 50K: 0
    \item 50K-100K: 1
    \item over 100K: 2
\end{itemize}
Then it is definitely the case that 2 $>$ 1 $>$ 0. And if we have an individual who earns, $>$ 1000 K, we give this the label 3, and these individuals are definitely rare and can be considered as outliers

\subsection{Outlier Detection and Scaling}
We try different scaling methods before we detect outliers, and we apply the outlier detection only for columns \textit{that are: numeric and/or ordinal}, and we extracted the percentage of outliers for each of the legal columns as well as the outlier values themselves (values considered outliers).

\noindent We realize that the outlier values, for the \textbf{ordinal} columns, happen to be the values \textit{8 and 9} which is normal because the values are either 0,1,2 or 8,9.

\subsection{Erroneous Codebook}
All the work done in Section \ref{sec:numeric} is summarized in the a codebook which we called: erroneous codebook. \footnote{The erroneous codebook can be found here on our public github repository: \url{https://github.com/hiyamgh/dementia/blob/master/input/codebooks/erroneous_codebook_legal_outliers_filtered.csv}\label{err_codebook}}

The following is a description of each column in the codebook:
\begin{enumerate}
\item \textbf{COLUMN:} Name of the feature
\item \textbf{data\_type:} type of the feature (specified manually by our team) numeric, ordinal, or categorical
\item \textbf{theme:} The survey theme of the feature. Extracted from the Copy of dementia baseline questionnaire. \footnote{Copy of dementia survey can be found here on our public github repository: \url{https://github.com/hiyamgh/dementia/blob/master/input/~\%24Copy\%20of\%20Dementia_baseline_questionnaire_V1.xlsx}\label{copyof_codebook}}
\item \textbf{description:} Description of the feature
\item \textbf{frequencies}: the unique values in the feature and their frequency (in percentage)
\item \textbf{perc\_missing:} percentage of missing values in the feature
\item \textbf{erroneous}: the erroneous values in the feature, if any.
\item \textbf{perc\_errnoneous:} percentage of erronous values, if any
\item \textbf{cut\_off:} cut off value based on which we selected the erroneous values in a feature. This was done manually by our team (refer to Section \ref{sec:markerrneous} for more details)
\item \textbf{outliers\_zscore:} percentage of outliers identified using z-score
\item \textbf{outliers\_iqr:} percentage of outliers identified using IQR 
\end{enumerate}

\subsection{Treating Erroneous Values}
Erroneous values are there due to intentional/un-intentional errors made by surveyors. However, they cannot stay in the dataset because this will lead to building machine learning models that learned from erroneous data. In order to avoid this, we have done the following:
\begin{enumerate}
\item Marked all erroneous values, in each feature, as being missing
\item Marked all erroneous values, in \textit{\textbf{ANIMALS\_2}} feature, with the value 10, which is the highest threshold.
\end{enumerate}


\subsection{Data Preprocessing}
Our legal features that we have filtered contain a mix of numeric, categorical, and ordinal features. By referring to the erroneous codebook \footref{err_codebook}, we have created all the features have missing values, we have done the following:


\subsubsection{Imputing Missing Values}
\begin{itemize}
\item \textbf{Numeric \& Ordinal} We have imputed missing values using KNN Imputation
\item \textbf{Categorical:} We have imputed missing values by creating a new category for the missing features. This decision is based on the fact that we cannot impute categorical features using KNN due to the discrete nature of categorical values.
\end{itemize}

\subsubsection{Scaling Features}
Due to the discrete nature of values in the categorical features, \textbf{we scaled only the numeric and ordinal features}. We have used Robust Scaling from python's \textit{sklearn} module. Robust Scaling is well suited for features with outlier and we do have outliers.

%\subsection{Feature Cross}
%Our data mostly consists of categorical/ordinal columns, and we have to find a way to solve these non-linearities such that it fairs better for the model's understanding of the features when we begin with the modelling phase. We generate the list of all possible feature crosses. 
%
%\noindent Feature crossing, will come after we 'one-hot encode' our data.


\section{Feature Selection}
\subsection{Decision Tree Feature Importance}
We use decision tree regressor to calculate feature importance implemented using scikit-learn's DecisionTreeRegressor.

After being fit, the model provides a feature\_importances\_ property that can be accessed to retrieve the relative importance scores for each input feature.

The results of the feature importance using the dummy imputed pooled data so far is then stored in the 'feature\_importance.csv', sorted from highest (highest importance score) to lowest (lowest importance score).

\subsection{Select K Best Features}
\subsubsection{Deciding "K"}
In order to select K best features, we need to first decide the value of K. In order to do this, we use sklearn's RFECV, which uses step forward recursive feature elimination (1-step) and cross validation to decide the number of features needed to give the highest out of sample accuracy. The model used for this was decision tree regressor.

\subsubsection{Selecting K Best Features}
After deciding on the value of K, which using the current imputation of the input data turned out to be 20, we perform step forward recursive feature elimination that ranks all the features as 0 or 1, with 1 meaning that a feature is one of the K best features. We store the names of the k-selected features (column names) into a csv file called "k\_best\_features.csv" This is also performed using a decision tree regressor.

\section{Oversampling and Undersampling}
Based on the examples discussed in the books, we implemented 4 functions that accept a model and add it to a pipeline after adding the oversampling/undersampling layers. The following are the functions created:

\begin{enumerate}
    \item random\_sampling: consists of a random oversampling and a random undersampling layers \textit{works with mixed data types}
    \item smote\_random\_sampling: adds a smote oversampler and a random undersampler \textit{works with mixed data types}
    \item smote\_tomek (combination approach): combines over and under sampling using SMOTE and Tomek links. \textit{could not find support for mixed datatypes}
    \item smote\_enn: combines over and under sampling using SMOTE and Edited Nearest Neighbours. \textit{could not find support for mixed datatypes}
\end{enumerate}

\subsection{Dealing With Mixed Variables}
The python library that is used in the book provides a model, SMOTENC, that handles mixed datatypes. For this model, the only difference is that we have to specify the indices of the categorical variables. The random sampler model can also be used for mixed datatypes. SMOTENC does not require scaling but according to my search it is adviced to scale the numerical variables using either MinMax or standard deviation scaling, and let the SMOTENC handle the categorical variables.

\section{Cost-Sensitive Learning}
\subsection{Cost sensitive learning consists of the following steps:}
\begin{enumerate}
    \item Input is the imputed and scaled data.
    \item Built a pipeline that consists of SMOTENC and the model of choice.
    \item Pipeline is passed into a GridSearchCV, to find the best percentage of sampling for the SMOTE, and the best params for the model of choice using GridSearch and cross validation. The scoring metric for finding the best combination of params was AUC ROC.
    \item cross validation was achieved through RepeatedStratifiedKFold%: the stratified k-fold cross-validation procedure is repeated n times, where the data sample is shuffled prior to each repetition, which results in a different split of the sample.
    %\item the above repeatedstratifiedkfold split is used in combination with a GridSearchCV where the scoring is the aurea under the ROC curve.
    \item The above steps are repeated for every tested model, followed by reporting the results of the test dataset.
    \item The results (metrics) are: ROC AUC, GMEANS, f2-score, BSS, AUC PR, cost matrix
    \item BSS: is calculated by using a reference score, and comparing the score of the model to that reference score (baseline score). BSS = 1 - (model BrierScore/ reference BrierScore). If the reference score was evaluated, it would result in a BSS of 0.0. This represents a no skill prediction. Values below this will be negative and represent worse than no skill. Values above 0.0 represent skillful predictions with a perfect prediction value of 1.0. The book suggests the reference score to be the brier score of a probability of 0.01 for all rows (not an actual model predicting the probabilities)
    \item The tested models are: XGBoost, KNeighbors Classifier, Balanced Random Forest (from the imbalanced library), Weighted Logistic Regression, Weighted Decision Tree,Weighted SVM
\end{enumerate}
\subsection{Tested sampling methods (using SMOTENC's sampling\_strategy options):}
\begin{enumerate}
    \item minority: resample the minority class (oversampling)
    \item not minority: resample the majority class (undersampling)
    \item all: resample both classes
    \item When float, it corresponds to the desired ratio of the number of samples in the minority class over the number of samples in the majority class after resampling, this ratio is reached by oversampling the minority class to reach the desired ratio. Tested ratios: 1, 0.5, 0.75
\end{enumerate}
\subsection{Tested class\_weights: (costs for each label), combination of suggestions from book and built-in options:}
In the below list, cost(1) means cost of false positive and cost(0) means cost of false negative.
\begin{enumerate}
    \item reverse: cost(1)= number of 0s in the train and cost(0) = number of 1s in the train. This is suggested by the book: invert the ratio of positive-to-negative and used as the cost of misclassification errors, where the cost of a False Negative is the length of negative in the dataset and the cost of a False Positive is the length of the negative in the original dataset. the book suggests that it is a good idea to use this heuristic as a starting point, then test a range of similar related costs or ratios to confirm it is sensible. 
    \item cost(1)= 10, cost(0)=1
    \item cost(1) = 100, cost(0)=1    
    \item cost(0)= 10, cost(1)=1
    \item cost(0) = 100, cost(1)=1
    \item balanced: The "balanced" mode provided by the library uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n\_samples / (n\_classes * np.bincount(y))
\end{enumerate}

\subsection{Probabilistic Models}
\subsubsection{Grid Search For Optimal Probability Threshold}
As suggested by the book, after performing grid search, training, and testing for each model, we test a range of probability thresholds to find the one that gives the highest f2 score. The range is from 0 to 1 with 0.001 steps.

\subsubsection{Calibrated Models}
This is provided by SKLEARN library, we specify class weights for each class in the label which allows for cost sensitive learning (as done in the book). Calibration was using sklearn's CalibratedClassifierCV, which takes a model and performs its regular grid search to find the best parameters but also uses cross validation to calibrate the model.

\subsubsection{Balanced Random Forest Results}
The Balanced Random Forest class from the imbalanced-learn library performs data sampling on the bootstrap sample
in order to explicitly change the class distribution and performs random undersampling of the majority class in each bootstrap sample. This is generally referred to as Balanced Random
Forest. It takes care of the class\_weight (cost matrix) and does not need it explicitly specified since it expects imbalanced datasets, Unlike other models provided by sklearn that need the cost matrix for imbalanced classification.

\subsection{Results - Probabilistic}

\subsubsection*{Top 10 - Optimization: F2}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_probabilistic_f2}}}
\caption{Results of probabilistic shallow models - taking top 10 features - optimized by f2}
\end{table}

\subsubsection*{Top 20 - Optimization: F2}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_probabilistic_f2}}}
\caption{Results of probabilistic shallow models - taking top 20 features - optimized by f2}
\end{table}

%%%%%%%%%%%%%%%%%% BSS %%%%%%%%%%%%%
\subsubsection*{Top 10 - Optimization: BSS}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_probabilistic_bss}}}
\caption{Results of probabilistic shallow models - taking top 10 features - optimized by bss}
\end{table}

\subsubsection*{Top 20 - Optimization: BSS}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_probabilistic_bss}}}
\caption{Results of probabilistic shallow models - taking top 20 features - optimized by bss}
\end{table}

%%%%%%%%%%%%%%%%%% GMEAN %%%%%%%%%%%%%
\subsubsection*{Top 10 - Optimization: GMEAN}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_probabilistic_gmean}}}
\caption{Results of probabilistic shallow models - taking top 10 features - optimized by gmean}
\end{table}

\subsubsection*{Top 20 - Optimization: GMEAN}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_probabilistic_gmean}}}
\caption{Results of probabilistic shallow models - taking top 20 features - optimized by gmean}
\end{table}

%%%%%%%%%%%%%%%%%% PR_AUC %%%%%%%%%%%%%
\subsubsection*{Top 10 - Optimization: PR\_AUC}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_probabilistic_pr_auc}}}
\caption{Results of probabilistic shallow models - taking top 10 features - optimized by pr\_auc}
\end{table}

\subsubsection*{Top 20 - Optimization: PR\_AUC}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_probabilistic_pr_auc}}}
\caption{Results of probabilistic shallow models - taking top 20 features - optimized by pr\_auc}
\end{table}

%%%%%%%%%%%%%%%%%% Sensitivity %%%%%%%%%%%%%
\subsubsection*{Top 10 - Optimization: Sensitivity}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_probabilistic_sensitivity}}}
\caption{Results of probabilistic shallow models - taking top 10 features - optimized by sensitivity}
\end{table}

\subsubsection*{Top 20 - Optimization: Sensitivity}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_probabilistic_sensitivity}}}
\caption{Results of probabilistic shallow models - taking top 20 features - optimized by sensitivity}
\end{table}

%%%%%%%%%%%%%%%%%% Specificity %%%%%%%%%%%%%
\subsubsection*{Top 10 - Optimization: Specificity}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_probabilistic_specificity}}}
\caption{Results of probabilistic shallow models - taking top 10 features - optimized by specificity}
\end{table}

\subsubsection*{Top 20 - Optimization: Specificity}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_probabilistic_specificity}}}
\caption{Results of probabilistic shallow models - taking top 20 features - optimized by specificity}
\end{table}

\subsection{Results - Classification}
%%%%%%%%%%%%%%%%%% Specificity %%%%%%%%%%%%%
\subsubsection*{Top 10 - Optimization: F2}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_10_regular_f2}}}
\caption{Results of classification shallow models - taking top 10 features - optimized by f2}
\end{table}

\subsubsection*{Top 20 - Optimization: F2}
\begin{table}[H]
\resizebox{.5\width}{!}{
\input{\detokenize{tables/maml/results/shallow/top_20_regular_f2}}}
\caption{Results of classification shallow models - taking top 20 features - optimized by f2}
\end{table}

\subsection{Results - One Class Classification}
\subsubsection*{Top 10 - Optimization: F2}
\begin{table}[H]
\input{\detokenize{tables/maml/results/shallow/top_10_one_class_f2}}
\caption{Results of one class classification shallow models - taking top 10 features - optimized by f2}
\end{table}

\subsubsection*{Top 20 - Optimization: F2}
\begin{table}[H]
\input{\detokenize{tables/maml/results/shallow/top_20_one_class_f2}}
\caption{Results of one class classification shallow models - taking top 20 features - optimized by f2}
\end{table}




\section{Encoding Categorical Data}
Several Methods exist for encoding categorical data. Methods could be divided into two: \textit{\textbf{unsupervised and supervised.}} We include the encoding method inside our grid search (for shallow models) and hyper parameter search (for the deep learning models). We will use only the \textbf{supervised categorical encoding methods} because they do not add dimensionality to the data. We have used the \textit{category\_encoders} python module, which supports all the variations of categorical encoding mechanisms. \footnote{\url{https://github.com/scikit-learn-contrib/category_encoders}}

\subsection*{Target Encoding}
In target encoding, we replace each category $X_i$ with the proportion of instances with that value that belong to the positive class. 

\subsection*{Catboost Encoder}
Similar to Target Encoding. Target Encoding introduces target leakage since the target is used to predict the target. Such models tend to be overfitted and don't generalize well in unseen circumstances.

\noindent A CatBoost encoder involves an ordering principle in order to overcome this problem of target leakage. It uses the principle similar to the time series data validation. The values of target statistic rely on the observed history, i.e, target probability for the current feature is calculated only from the rows (observations) before it. Categorical feature values are encoded using the following formula:
\begin{equation}
\frac{\text{TargetSum} + \text{prior}}{\text{FeatureCount} + 1}
\end{equation}

\noindent \textbf{TargetCount}:  Sum of the target value for that particular categorical feature (upto the current one).

\noindent \textbf{Prior}: It is a constant value determined by (sum of target values in the whole dataset)/(total number of observations (i.e. rows) in the dataset)

\noindent \textbf{FeatureCount}: Total number of categorical features observed upto the current one with the same value as the current one.\footnote{\url{https://www.geeksforgeeks.org/categorical-encoding-with-catboost-encoder/}}

\subsection*{James-Stein Estimator}
It is similar to target encoding, but better. Lets say for example that 4\% of population of 4000 are color blind versus 4\% of population of 50 are color-blind. In the first case, we are confident that the proportion is close to 4\%, but in the latter case, only about 2 out of 50 are color-blind. These 2 might create a lot of noise in the data.

\noindent The James-Stein encoder shrinks the average toward the overall average. If $p_{\text{all}} $ is the overall proportion of people that are color-blind in our sample set, we have

\begin{equation}
\text{Encoded value for group}  i = (1 - B)p_i + Bp_{\text{all}} 
\end{equation}

\noindent Where $B$ is a weight of the population mean, and $1-B$ is the weight of the group mean (with the total weight being 1).

\noindent There are different methods for calculating B, as discussed in the documentation, but the default one in category encoders is called the "independent model". For each category we have:

\begin{equation}
B = \frac{(\text{group variance})}{(\text{group variance}) + (\text{popualtion variance})}
\end{equation}


\noindent When we are uncertain about a group's value (i.e. the group variance is high compared to the population variance) then $B \approx 1$, and we are heavily biased toward the population value. When the group variance is much lower that the population variance, $B \approx 0$ and we use the value for the group instead. \footnote{\url{https://kiwidamien.github.io/james-stein-encoder.html}}

\subsection*{M-Estimator}
 a simplified version of Target Encoder. The stands for maximum likelihood-type. It has only one hyper-parameter $m$, which represents the power of regularization. The higher the value of $m$ results into stronger shrinking. Recommended values m are in the range of 1 to 100 \footnote{\url{https://towardsdatascience.com/how-to-encode-categorical-data-d44dde313131}}

\subsection*{Weight of Evidence}
The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers. "Bad Customers" refers to the customers who defaulted on a loan. and "Good Customers" refers to the customers who paid back loan.

\begin{equation}
WOE = \ln{\frac{\text{Distribution of Goods}}{\text{Distribution of Bads}}}
\end{equation}

\noindent In the case of binary classification, it will become:

\begin{equation}
WOE = \ln{\frac{p(1)}{p(0)}}
\end{equation}

\noindent Where $p(1)$ is the probability of the target being 1 (positive class) and $p(0)$ is the probability of the target being 0 (negative class). 

\noindent For every category in a feature, calculate $p(0)$ \& $p(1)$, compute the natural logarithm, and replace. \footnote{\url{https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html}}\footnote{\url{https://heartbeat.fritz.ai/hands-on-with-feature-engineering-techniques-encoding-categorical-variables-be4bc0715394}}

\subsection*{Generalized Linear Mixed Model}
supervised encoder similar to Target Encoder or M-Estimator, but there are some advantages:
\begin{enumerate}
\item Solid statistical theory behind the technique. Mixed effects models are a mature branch of statistics.
\item No hyper-parameters to tune. The amount of shrinkage is automatically determined through the estimation process. In short, the less observations a category has and/or the more the outcome varies for a category then the higher the regularization towards "the prior" or "grand mean"
\item The technique is applicable for both continuous and binomial targets. If the target is continuous, the encoder returns regularized difference of the observation’s category from the global mean. If the target is binomial, the encoder returns regularized log odds per category.
\end{enumerate}

In comparison to James-Stein Estimator, this encoder utilizes generalized linear mixed models from \textit{statsmodels} library \footnote{\url{https://contrib.scikit-learn.org/category_encoders/glmm.html}}

\section{SHAP}
\subsection{Global SHAP Summary Values Results}
The summary plot combines feature importance with feature effects. Each point on the summary plot is a Shapley value for a feature and an instance. The position on the y-axis is determined by the feature and on the x-axis by the Shapley value
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{\detokenize{shap_plots/1.png}}
\caption{Class 0}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{\detokenize{shap_plots/2.png}}
\caption{Class 1}
\end{figure}
\subsection{Global SHAP Interaction Values}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{\detokenize{shap_plots/3.png}}
\end{figure}
\subsection{Multi-output Decision Plot for properly classified Rows}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{\detokenize{shap_plots/4.png}}
\caption{Row with label = 0}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{\detokenize{shap_plots/5.png}}
\caption{Row with label = 1}
\end{figure}
\subsection{Decision Plot for Miss-classified Row}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{\detokenize{shap_plots/6.png}}
\end{figure}

\section{Chapter 6: Precision, Recall, and F-measure}
The chapter summarizes the precision, recall, and f-measures of multiclass and binary classification for the case of balanced data.

\section{Chapter 7:ROC Curves and Precision-Recall
Curves}
ROC Curves (receiver operating characteristic curve) and ROC AUC (area under the ROC curve) are used to decide if the model differentiates between the labels. 
\subsection{ROC Curve}
The threshold is applied to the cut point in probability between the positive and negative classes. In order to decide on the best threshold, we evaluate the true positive and false positives for different threshold values, a curve can be constructed that stretches from the bottom left to top right and bows toward the top left. This curve is called the ROC curve. A classifier that has no discriminative power between positive and negative classes will form a diagonal line between (0,0) and (1,1). Models represented by points below this line have worse than no skill.
\subsection{AUC}
Area under the curve is calculated to give a single score for a classifier model across all threshold values. This is called the ROC area under curve or ROC AUC or sometimes ROCAUC. The score is a value between 0.0 and 1.0, with 1.0 indicating a perfect classifier.
\section{Chapter 8: Probability Scoring Methods}
\subsection{Probability Metrics}
On some problems, a crisp class label is not required, and instead a probability of class membership is preferred. The probability summarizes the likelihood (or uncertainty) of an example belonging to each class label. 
\subsection{LogLoss Score}
Logarithmic loss or log loss for short is a loss function known for training the logistic regression classification algorithm. The log loss function calculates the negative log likelihood for probability predictions made by the binary classification model. Most notably, this is logistic regression,but this function can be used by other models, such as neural networks, and is known by other names, such as cross-entropy.
\section{Cross Validation for Imbalanced Datasets}
The solution is to not split the data randomly when using k-fold cross-validation or a train-test split. Specifically, we can split a dataset randomly, although in such a way that maintains the same class distribution in each subset. This is called stratification or stratified sampling and the target variable (y), the class, is used to control the sampling process. This is available using sklearn stratified kfold.
\section{Chapter 10}
Summarizes oversampling and undersampling techniques which are available in chapters 12 and 13 in details.
\section{Chapter 12}
\subsection{Oversampling}
All techniques available through the python library: imblearn.over\_sampling

\begin{itemize}
    \item Random Oversampling: simplest oversampling method, involves randomly duplicating examples from the minority class in the training dataset
    \item SMOTE (Synthetic Minority Oversampling Technique): works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample as a point along that line
    \item Borderline-SMOTE: involves selecting those instances of the minority class that are misclassified, such as with a k-nearest neighbor classification model, and only generating synthetic samples that are difficult to classify. Borderline Oversampling is an extension to SMOTE that fits an SVM to the dataset and uses the decision boundary as defined by the support vectors as the basis for generating synthetic examples, again based on the idea that the decision boundary is the area where more minority examples are required.
    \item Adaptive Synthetic Sampling (ADASYN): another extension to SMOTE that generates synthetic samples inversely proportional to the density of the examples in the minority class. It is designed to create synthetic examples in regions of the feature space where the density of minority examples is low, and fewer or none where the density is high.
\end{itemize}
\subsection{Undersampling Techniques}
\begin{itemize}
    \item Random Undersampling
\end{itemize}
\section{Data Transforms (Hiyam)}
\subsection{Scaling Numeric Data (chapter 17)}
\begin{itemize}
\item Normalization (Min-Max Scaling)
\item Standardization
\end{itemize}

\subsection{Scaling Data with Outliers (chapter 18)}
Many machine learning algorithms perform better when \textbf{numerical} input variables are scaled. Standardization is a popular scaling technique that substracts the mean from the values and divide by the stadard deviation, transforming the probability distribution from for an input variable to a Standard Gaussian (zero mean and unit variance).

\noindent \textbf{Problem:} Standardization can become skewed or biased when the input variable contains outliers.

\subsubsection*{Robust Scaling}
When we are scaling the data, and if we have very large values relative to the other input variables, these large values can dominate or skew some machine learning algorithms. \textbf{The result is that the algorithms pay most of their attention on the large values and ignore the variables with smaller values}.

\noindent Outliers are values at the edge of the distribution that may have a low probability of occurence, yet are overrepresented for some reason. \textbf{Outliers can skew a probability distribution and make data scaling using standardization difficult as the calculated mean and standard deviation will be skewed by the presence of outliers.}

\noindent \textbf{Approach:} When standardizing input variables containing outliers, we can ignore outliers from the calculation of the mean and standard deviation, and use the calculated values to scale the data

\noindent \textbf{This is called robust standardization}. This can be achieved by calculating the median (50th percentile) and the 25th and 75th percentile. The values of each variable can then have their median subtracted and are divided by the inter quartile range (IQR) which is the difference between the 25th and 75th percentile.

\begin{equation}
value = \frac{value - median}{p_{75} - p_{25}}
\end{equation}

The resulting value has a \textbf{zero mean and median and a standard deviation of 1}. Although not skewed by outliers and the outliers are still present with the same relative relationships to other values.

\subsection{How to Encode Categorical Data (chapter 19)}
Machine learning models require all input and output variables to be numeric. This means that if the data contains categorical data, we must encode it to numbers before we fit and evaluate our models.

\subsection*{Ordinal Encoding}
Each unique category is assigned an integer value. Example: \textit{red} is 1, \textit{green} is 2, \textit{blue} is 3.

\noindent For categorical variables, it imposes and \textbf{ordinal relationship} when no such relationship exists. This may cause problems and \textbf{one hot encoding} may be used instead.

\subsection*{One Hot Encoding}
For categorical variable where no ordinal relationship exists, the ordinal encoding is not enough and may be misleading.

\noindent One Hot Encoding works by creating binary variables for each category. For Example: \textit{red} will be [1, 0, 0], \textit{blue} will be [0, 1, 0], and \textit{green} will be [0, 0, 1].

\subsection*{Dummy Variable Encoding}
The one hot encoding includes a binary representation for each category. This might cause redundancy. 

\noindent if we know that [1, 0, 0] represents \textit{blue}, and [0, 1, 0] represents \textit{green}, we don't need another binary variable to represent \textit{red}, instead we could use 0 values along, e.g. [0, 0]. This is called dummy variable encoding and always represents $C$ categories with $C-1$ binary variables.

\noindent Other being less redundant, it is required for some models.

\subsection{How to Make Distributions Look More Gaussian (chapter 20)}
Several Machine Learning Algorithms assume the numerical values have a Gaussian distribution. Our data may not have a Gaussian distribution and it might have a Gaussian-like distribution.

\section{Data Pre-Pocessing}
After we have filtered the legal features in our data, and we deleted the features that have $>$ 40 \% missing values, we are left out with 255 legal features. These features must be pre-processed before we do the modelling exercise, for the following reasons:
\begin{enumerate}
\item All the 255 legal features still have missing values
\item We have a combination of numeric, ordinal, and categorical features, whereby each might require a different kind of scaling
\end{enumerate}

\subsection{Imputing Missing Values - Categorical Features}
For \textbf{all} categorical features, we will impute missing values by adding \textbf{a new category} for missing values. This way, any value that is missing will have its own category(label)

\subsection{Imputing Missing Values - Numerical/Ordinal Features}
For all numeric/ordinal features, we will impute missing values using \textbf{KNN}.

\noindent \textcolor{blue}{We discussed in one of our meeting that we will treat ordinal features as numeric, that's why we considered treating ordinal as numeric features therefore they are also imputed using KNN}

\subsection{Scaling - Numeric/Ordinal Features}
We applied \textbf{Robust Scaling} for all numeric/ordinal features.

\noindent \textbf{Robust Scaling} is good for data that includes outliers, and all our features do have outliers (with a very low percentage, though). 

\noindent For more information about the \textbf{percentage of outliers/feature, please consult with the following dataframe on our github repository: \url{https://github.com/hiyamgh/dementia/blob/master/input/codebooks/erroneous_codebook_legal_outliers_filtered.csv}}

\subsection{Scaling - Categorical Features}
No scaling is applied for categorical features.


% \subsection{Imputation in Ordinal/Categorical Data}
% To be continued ... waiting to meet with Dr Khalil again. Some notes:
% \begin{enumerate}
%     \item We said we might do KNN and impute by getting the values from the nearest neighbors.
%     \item It is common that public health practitioners usually disregard features with greater than 25\% missing, and since we have features that have greater than 25\% missing, we got the \textbf{theme} of each feature, and if the feature belongs to a particular theme that might not be of vital importance for detecting dementia, we will dis-regard the feature by itself.  
% \end{enumerate}

% \subsection{Imputation Currently Done}
% Just for now, to get things going, we imputed all missing values by replacing with the majority

% \subsection{Scaling}
% We have said earlier that we have three data types in our features: \textit{\textbf{numeric, ordinal, and categorical.}}

% \begin{itemize}
%     \item For ordinal and numeric data, we will be using the \textbf{Robust scaling} mechanism due to the presence of outliers
%     \item For categorical data, we will not scale it as we will be applying \textbf{dummy variable encoding} 
% \end{itemize}


% \subsection{Encoding Data}
% \begin{itemize}
%     \item For ordinal features, we will be applying Ordinal Encoding, as it will maintain an order between feature values.
%     \item For categorical features, we apply dummy variable encoding that decreases redundancy of one hot encoding
% \end{itemize}


\section{Imputing Numerical Missing Data}
In this section, we will be listing the multiple imputation methods that may be used for imputing missing \textbf{numeric} data. \textcolor{blue}{I referred to the data preparation book we have}

\subsection{Statistical Imputation}
Using mean, median, mode, constant value

\subsection{K Nearest Neighbors}
K nearest neighbor model. A new sample is imputed by finding samples in the training set closest to it and averages these nearby points to fill in the value. Must specify the \textbf{distance} to use and the \textbf{number of neighbors} neeeded for imputation

\subsection{Iterative Imputation}
.  Iterative imputation refers to a process where each feature is modeled as a function of the other features, e.g.  a regression problem where missing values are predicted.  Each feature is imputed sequentially, one after the other,allowing prior imputed values to be used as part of a model in predicting subsequent features.It is iterative because this process is repeated multiple times, allowing ever improved estimates of missing values to be calculated as missing values across all features are estimated.  \textcolor{blue}{This approach may be generally referred to as fully conditional specification (FCS) or multivariate imputation by chained equations (MICE)}.

\section{Imputing Categorical Missing Data}
In this section, we will be listing the multiple imputation methods that may be used for imputing missing \textbf{categorical} data

\section*{Single based Imputation Methods}

\subsection{Mean Imputation}
The mean imputation replaces missing values with the observed mean of the available data.

\subsection{Imputation Using Most Frequent or (Zero/Constant Values)}
Most Frequent is another statistical strategy to impute missing values and it works with categorical features (strings or numerical representations) by replacing missing data with the most frequent values within each column

\subsection{Create a New Category (Random Category) for NAN Values}
\textcolor{blue}{I think its more of a "hack" rather than an actual imputation method}

\subsection{ Adding a Variable To Capture NAN}
Replace NAN categories with most occurred values, and add a new feature to introduce some weight/importance to non-imputed and imputed observations. Create a new column and replace 1 if the category is NAN else 0. This column is an importance column to the imputed category.Replace NAN value with most occurred category in the actual column.

\subsection{Imputation Using Deep Learning (Datawig)}
\textcolor{blue}{Its actually a repository for AWS -- DataWig learns Machine Learning models to impute missing values in tables. -- So not sure how trustworthy it is}
This method works very well with categorical and non-numerical features. It is a library that learns Machine Learning models using Deep Neural Networks to impute missing values in a dataframe. It also supports both CPU and GPU for training.

\subsection{Regression Imputation}
\textcolor{blue}{The resource I used mentions this under the imputation for categorical variables, unless they treat each category as a \textbf{regressor variable}(numeric) but in my humble opinion, I think it can be used as a classification (in case we want to predict a category rather than a numeric outcome)?}
Regression Imputation consists of using some selected prediction of a missing value  on a variable of interest. For instance, to predict the missing value for the variable, say $X_1$, use this variable as a function of other variables, say $X_2$ and $X_3$ in a model that could even include the dependent variable $Y$

\textbf{Cons:} The \textbf{uncertainty} is not incorporated very well because the estimates are random variables.

\subsection{Imputation Using Interpolation}
\textcolor{blue}{I think this might fit if the data is time series but I'm not sure if it can be used in non-time series data} Suppose a varoable $X$ is measured at times $t = 1, 2, 3 (X_1, X_2, and X_3)$ and some values are missing at $t=2 (X_2)$, then $X_2 = \frac{X_1 + X_3}{2}$

\subsection*{Multiple Imputation}
Single based Imputation Methods mentioned earlier are an improvement over the case deletion method, but they do not account for the uncertainty in the imputations as imputed values are treated as true rather than estimates of the missing values leading to the under estimation of the variance of the estimates and the distortion of relationships among variables.Goal of \textbf{multiple imputation} is to account for uncertainty in imputed values. This method uses a selected model, such as a regression model to predict missing values on a variable. Instead of picking one value for the missing value, many values are chosen and the uncertainty is presented in the variance covariance matrix.

\subsection{Multivariate Normal Imputation (MNVI)}
\begin{itemize}
\item MNVI assumes all variables in the model are normally distributed \textcolor{blue}{problem for us?}
\item uses Markov Chain Monte Carlo Procedure to obtained imputed values from estimated multivariate distribution allowing uncertainty
\end{itemize}

\subsection{Multiple Imputation by Chained Equations (MICE)}
This type of imputation works by filling the missing data multiple times. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds or survey skip patterns. 

\subsection*{Other Imputation Methods}
\subsection{Stochastic regression imputation:}
It is quite similar to regression imputation which tries to predict the missing values by regressing it from other related variables in the same dataset plus some random residual value.

\subsection{Extrapolation and Interpolation:}
It tries to estimate values from other observations within the range of a discrete set of known data points.

\subsection{Hot-Deck Imputation}
Works by randomly choosing the missing value from a set of related and similar variables.

\subsection{Some papers for handling missing categorical data (did not read them at length -- just the abstract)}

\begin{itemize}
\item \href{https://www.sciencedirect.com/science/article/abs/pii/S0164121216301583?casa_token=ZELiCdTNfw4AAAAA:_dpjT3fvcxOkRXX1po7HaCnzb02biPjrKEYw-j8d7OU_krEYFqaaQIIwQo3rki5tbUWitsJstA}{Combining instance selection for better missing value imputation}
\item \href{https://link.springer.com/article/10.1007/s10115-019-01427-1}{Missing data imputation using decision trees and fuzzy clustering with iterative learning}
\item \href{https://www.sciencedirect.com/science/article/abs/pii/S0925231216309407?casa_token=dC_ROkr-pWoAAAAA:s5zwIWUilFRt2zbv5YcFVu6kzsNhdZN3c_OfUpQ1s1e-f9-c4VqcB1_OnXS-kGF0-kPzctFJ9A}{Probabilistic neural network based categorical data imputation}
\end{itemize}

\subsection*{References}
\begin{itemize}
\item \url{https://projecteuclid.org/download/pdfview_1/euclid.bjps/1481619615}

\item \url{http://www.jds-online.com/files/JDS-612.pdf}

\item\url{ https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779}
\item \url{https://github.com/awslabs/datawig}
\item \url{https://medium.com/analytics-vidhya/ways-to-handle-categorical-column-missing-data-its-implementations-15dc4a56893}
\end{itemize}




\section{Plan for the Paper}
\begin{enumerate}
\item Fix roaa’s code
\item MAML with random sampling, Reptile with random sampling
\item MAML with FP growth, Reptile with FP growth
\item MAML with ARML, Reptile with ARML
\item Ensure process is thorough with hyperparameter search documented, winning hyper-parameters documented, and performance metrics cast against the baseline model
\item Quantitative Comparison between baseline ML, vanilla ML, metalearning in three sampling procedures
\item 
Qualitative Comparison between baseline ML, vanilla ML, metalearning in three sampling procedures
\end{enumerate}


\section{MAML - Sampling Training/Testing Tasks}
The MAML code is divided into a main method that calls the appropriate training and testing methods. Each method, will appropriately call the data generator component, which is responsible for generating the tasks that the meta learner uses for training and testing respectively. \textbf{The data generation process depends on the following hyper parameters:}
\begin{itemize}
\item \textit{meta\_batch\_size:} Number of tasks sampled per meta-update
\item \textit{update\_batch\_size:} Number of tasks samples per meta-update
\item \textit{num\_classes:} number of classes we are classifying
\end{itemize}

\noindent\textbf{The data generator component takes in two parameters as input:}
\begin{itemize}
\item \textit{num\_samples\_per\_class:} number of samples (i.e. data instances) to sample per classification class. (i.e. how many data samples we will take from each class. If $\text{num\_samples\_per\_class}= 32$, then, we will take 32 samples from class 0, and 32 samples from class 1. Notice this is a \textbf{stratified} sampling.)
\item \textit{batch\_size:} the batch size, whereby each mini-batch will contain the appropriate number of samples. 
\end{itemize}

\noindent\textbf{Note (Very Important):} Each mini-batch (from "\textit{batch\_size}") will contain  $  \text{num\_classes} * \text{num\_samples\_per\_class}$ data samples. The main method in the original MAML code that was public on github (which we took and adapted changes as appropriate) \footnote{\url{https://github.com/cbfinn/maml}} called the Data Generator component by setting the data generation parameters parameters to be equal to the following:
\begin{itemize}
\item $\text{num\_samples\_per\_class} = \text{update\_batch\_size} * 2$
\item $\text{batch\_size} = \text{meta\_batch\_size}$
\end{itemize}

\subsection*{Meta-Training}
\noindent Therefore, if for example we have $\text{meta\_batch\_size} = 16$ and $\text{update\_batch\_size}=16$, then the $\text{batch\_ size} = 16 $ \& $\text{num\_samples\_per\_class} = 16 * 2 = 32$. By that, since each mini-batch will have $n\text{num\_samples\_per\_class}$ number of data samples, then we have in total $16 * 32 = 512$ data samples used for training, and $512$ samples \textbf{for each class}, therefore, a total of $512 + 512 = 1024$ samples.

\subsection*{Support \& Query Sets}
We have seen in the example above that we had 1024 (16 batch size, 32 samples per class, 2 classes) training samples. These are distributed into \textit{support and query sets}. 

\noindent Similar to the idea of training, validation, and testing in a regular supervised learning exercise, here in meta learning, the support set is what the meta learner trains on, and the query set is what the meta learner validates on.

\noindent Each mini-batch will contain a support set, and a query set. Mini-batch is called an \textbf{\textit{episode}}. Since each episode contains 64 samples in total (32 for each class), those 64 samples, are further distributed between support and query sets. The support will take 32 samples and the query will take 32 samples (stratified distribution).

\subsection*{N-way K-shot}
The number of classes in the support set, is referred to as \textit{N-way} task, and the number of samples per class is referred to as \textit{K-shot}. Therefore, our task is a \textit{\textbf{2-way 32-shot classification task}} 


\subsection*{How Do We Evaluate Testing Predictions ?}
In the MAML inner loop, we calculate the gradient updates of all weights and pass the updated weights to the model to get the new prediction. The query dataset is used to get the task losses after each gradient update. However, we do multiple gradient updates, and this is controlled by the parameter $num\_inner\_updates$ \footnote{\url{https://cs330.stanford.edu/material/cs330_hw2.pdf}}. Therefore, when we report the testing error metrics, we report the performance on the support set and the performance on the query set. Since the query set is used in while doing gradient updates, and since we do that "$num\_inner\_updates$" times, we report the performance averaged across those "$num\_inner\_updates$" for the query set. 

 
\noindent In other terms, we have 1024 (16 meta batch size, 32 examples per class, 2 classes) distributed as 512 support set samples, and 512 query set samples. However, the 512 samples are tested $\text{num\_test\_updates} = 10$ times, \textbf{therefore, we are reporting the average of the support set + the 10 query set updates.}


\section{MAML vs Shallow Models - Dementia Dataset}

\subsection{Cost Sensitive Learning in Neural Networks}
We have many types of cost sensitive learning, here, we will be focusing on two types which are \textbf{weighting} and \textbf{miss-classification error/cost matrix}.

It is important to note that, after the input is forwarded from the input layer to the output layer, the neural network computes the \textbf{loss} which quantifies how much mistake did it make, if any. 

\subsection*{Key defenition - Loss}
The loss function in case of regression is the \textbf{mean squared error}, whilst that for classification is the \textbf{cross entropy loss} (binary cross entropy (for binary classification tasks) or categorical cross entropy for multi-classification tasks)

\subsection*{Key defenition - Logits}
Loss if computed on the \textbf{logits}. Logits are the raw outputs of the last layer of the neural network. Logits interpreted to be the unnormalised (or not-yet normalised) predictions (or outputs) of a model. These can give results, but we don't normally stop with logits, because interpreting their raw values is not easy.

Let's demonstrate this through an example, say we want to classify images as being either a cat or a dog. For the first new image, we get logit values out of 16.917 for a cat and then 0.772 for a dog. Higher means better, or ('more likely'), so we'd say that a cat is the answer. The correct answer is a cat, so the model worked. If we apply softmax activation to the logits, we get p(cat) = 0.99 and p(dog) = 0.0001

\subsection*{Weighting}
Weighting is applied to the loss such that smaller weight values result in errors, i.e. less update the model coefficients and larger weight results in more errors, i.e. more updates to the model coefficients.

\textbf{Multiply the logit with a weight vector representing scaling factor of each class}. 

\subsection*{Miss-Classification Errros/Cost-Matrix}
The problem with class weighting is that it applies the weighting to all the data that belongs to the class, whilst we want it to depend on the miss classification error.

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
actual/predicted & class 0 & class 1 \\ \hline
class 0          & 0       & 0.25    \\ \hline
class 1          & 0.25    & 0       \\ \hline
\end{tabular}
\caption{cost matrix of classifications/miss-classifications}
\end{table}

The table above shows that the cost of miss-classifying class 0 as class 1 or classifying class 1 as class 0 is 0.25 while correct classifications have no cost.

%%%%%%%%%%%%%%%%%%%%%%% Hyper Parameter Space %%%%%%%%%%%%%%%%%%
\subsection{Hyper Parameters}
In this section, we will be displaying the set of all \textbf{meta learning} related hyper parameters in comparison to the values chosen by the authors of MAML in their experiments:

\begin{table}[H]
%\resizebox{.7\width}{!}
{\input{\detokenize{tables/maml/hyperparameters/meta_learning_hyperparameters}}}
\caption{Hyper Parameter Space Containing all Meta-Learning Related Hyper Parameters}
\label{Tab:maml_meta_learning_hyperparameters}
\end{table}

\begin{table}[H]
%\resizebox{.7\width}{!}
{\input{\detokenize{tables/maml/hyperparameters/other_hyperparameters}}}
\caption{Hyper Parameter Space Containing all other Hyper Parameters related to Base Model, Cost Sensitive Learning, and Categorical Encoding}
\label{Tab:maml_other_hyperparameters}
\end{table}

It is important to note that:
\begin{enumerate}
\item \textbf{step size $\alpha$:} is represented as the \textbf{step size (alpha) - update learning rate} in Table \ref{Tab:maml_meta_learning_hyperparameters} above.
\item \textbf{meta step size $\beta$:} is represented as the \textbf{meta step size - meta learning rate} in Table \ref{Tab:maml_meta_learning_hyperparameters} above.
\item \textbf{K}: the number of samples $K$ denoting N-way K-shot classification is represented as the \textbf{update batch size - K (number of examples per class)} in Table \ref{Tab:maml_meta_learning_hyperparameters} above
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%% Winning Hyper Parameters %%%%%%%%%%%%%%%%%%
\subsection{Winning Hyper Parameters}
In Tables \ref{Tab:winning_with_top10},\ref{Tab:winning_with_top20},\ref{Tab:winning_without_top10}, and \ref{Tab:winning_without_top20} below, we display the winning hyper parameters for the top 20 best MAML models trained and tested on dementia top 10 and top 20 columns selected by feature selection, with and without including sampling with frequent patterns, respectively. Each table contains the following information:
\begin{itemize}
\item \textbf{model:} the name of the winning model
\item \textbf{miter:} the meta training iterations 
\item \textbf{mbs:} the meta batch size
\item \textbf{mlr:} the meta learning rate
\item \textbf{ulr:} the update learning rate
\item \textbf{dh:} dimensions hidden, i.e. the number of hidden layers and the number of nodes in each hidden layer. For example: \textit{128, 64} means that we have 2 hidden layers, the first having 128 nodes and the second having 64 nodes.
\item \textbf{afn:} activation function.
\item \textbf{nu:} number of testing updates. This is used by the meta learning code to repeat the evaluation on the query set, so here in this case, we evaluate the same query set 4 times for checking the generality of the results.
\item \textbf{ifp:} a boolean variable of whether we are including sampling with frequent patterns or not. 1 means we are, 0 means we are not.
\item \textbf{fp\_supp:} the minimum support used for discovering the frequent patterns.
\item \textbf{encoding:} The categorical encoding mechanism applied for the categorical columns in the data
\end{itemize}


% winning - with fp - top 10
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/winning_hyperparameters/winning_with_top10}}}
\caption{Winning Hyper Parameters - MAML with FP, using top 10 columns}
\label{Tab:winning_with_top10}
\end{table}

% winning - with fp - top 20
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/winning_hyperparameters/winning_with_top20}}}
\caption{Winning Hyper Parameters - MAML with FP, using top 20 columns}
\label{Tab:winning_with_top20}
\end{table}


% winning - without fp - top 10
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/winning_hyperparameters/winning_without_top10}}}
\caption{Winning Hyper Parameters - MAML without FP, using top 10 columns}
\label{Tab:winning_without_top10}
\end{table}

% winning - without fp - top 20
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/winning_hyperparameters/winning_without_top20}}}
\caption{Winning Hyper Parameters - MAML without FP, using top 20 columns}
\label{Tab:winning_without_top20}
\end{table}


\subsection{Quantitative Results - Without FP Growth}

\subsubsection*{Without FP - Top 10 - F2 score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top10_f2}}}
\caption{Results - MAML without FP, using top 10 columns - aggregated by f2 score}
\label{Tab:without_fp_top10_f2}
\end{table}

\subsubsection*{Without FP - Top 10 - BSS score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top10_bss}}}
\caption{Results - MAML without FP, using top 10 columns - aggregated by bss score}
\label{Tab:without_fp_top10_bss}
\end{table}

\subsubsection*{Without FP - Top 10 - GMEAN score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top10_gmean}}}
\caption{Results - MAML without FP, using top 10 columns - aggregated by gmean score}
\label{Tab:without_fp_top10_gmean}
\end{table}

\subsubsection*{Without FP - Top 10 - PR\_AUC score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top10_pr_auc}}}
\caption{Results - MAML without FP, using top 10 columns - aggregated by PR\_AUC score}
\label{Tab:without_fp_top10_pr_auc}
\end{table}

\subsubsection*{Without FP - Top 10 - Sensitivity score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top10_sensitivity}}}
\caption{Results - MAML without FP, using top 10 columns - aggregated by sensitivity score}
\label{Tab:without_fp_top10_sensitivity}
\end{table}

\subsubsection*{Without FP - Top 10 - Specificity score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top10_specificity}}}
\caption{Results - MAML without FP, using top 10 columns - aggregated by specificity score}
\label{Tab:without_fp_top10_specificity}
\end{table}


\subsubsection*{Without FP - Top 20 - F2 score}
% without fp - top 20 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top20_f2}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by f2 score}
\label{Tab:without_fp_top20_f2}
\end{table}


\subsubsection*{Without FP - Top 20 - BSS score}
% without fp - top 20 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top20_bss}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by bss score}
\label{Tab:without_fp_top20_bss}
\end{table}

\subsubsection*{Without FP - Top 20 - GMEAN score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top20_gmean}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by gmean score}
\label{Tab:without_fp_top20_gmean}
\end{table}

\subsubsection*{Without FP - Top 20 - PR\_AUC score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top20_pr_auc}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by PR\_AUC score}
\label{Tab:without_fp_top20_pr_auc}
\end{table}

\subsubsection*{Without FP - Top 20 - Sensitivity score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top20_sensitivity}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by sensitivity score}
\label{Tab:without_fp_top20_sensitivity}
\end{table}

\subsubsection*{Without FP - Top 20 - Specificity score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/without_fp_top20_specificity}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by specificity score}
\label{Tab:without_fp_top20_specificity}
\end{table}



\subsection{Insights}
\begin{enumerate}
\item We realize that the highest F2 score reached was by an MAML model, that \textbf{did not} incorporate FP Growth, and taking the top 20 columns selected by feature selection. \textbf{The highest F2 score is 0.91 shown in Table \ref{Tab:without_fp_top20_f2}}

\item We realize that the highest BSS score reached by an MAML model, that \textbf{did not} incorporate FP Growth, and taking the top 20 columns selected by feature selection.\textbf{ The highest  BSS score is 0.63 shown in Table \ref{Tab:without_fp_top20_bss}}

\item We realize that the highest GMEAN score reached by an MAML model, that \textbf{did not} incorporate FP Growth, and taking the top 20 columns selected by feature selection.\textbf{ The highest  GMEAN score is 0.81 shown in Table \ref{Tab:without_fp_top20_bss}}

\item We realize that the highest PR\_AUC score reached by an MAML model, that \textbf{did not} incorporate FP Growth, and taking the top 20 columns selected by feature selection. \textbf{The highest  PR\_AUC score is 0.76 shown in Table \ref{Tab:without_fp_top20_bss}}
\end{enumerate}

\subsection{Quantitative Results - With FP Growth}

\subsubsection*{With FP - Top 10 - F2 score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/with_fp_top10_f2}}}
\caption{Results - MAML with FP, using top 10 columns - aggregated by f2 score}
\label{Tab:with_fp_top10_f2}
\end{table}

\subsubsection*{With FP - Top 10 - BSS score}
% without fp - top 10 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/with_fp_top10_bss}}}
\caption{Results - MAML with FP, using top 10 columns - aggregated by bss score}
\label{Tab:with_fp_top10_bss}
\end{table}


\subsubsection*{With FP - Top 20 - F2 score}
% without fp - top 20 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/with_fp_top20_f2}}}
\caption{Results - MAML with FP, using top 20 columns - aggregated by f2 score}
\label{Tab:with_fp_top20_f2}
\end{table}


\subsubsection*{With FP - Top 20 - BSS score}
% without fp - top 20 - bss
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/maml/results/not_shallow/with_fp_top20_bss}}}
\caption{Results - MAML without FP, using top 20 columns - aggregated by bss score}
\label{Tab:with_fp_top20_bss}
\end{table}


\subsection{Insights}
\begin{enumerate}
\item We realize that the highest F2 score reached was by an MAML model, that \textbf{incorporated} FP Growth, and taking the top 10 columns selected by feature selection. \textbf{The highest F2 score is 0.91 shown in Table \ref{Tab:with_fp_top20_f2}}

\item We realize that the highest BSS score reached by an MAML model, that \textbf{incorporated} FP Growth, and taking the top 10 columns selected by feature selection. \textbf{The highest  BSS score is 0.67 shown in Table \ref{Tab:with_fp_top10_bss}}


\item We realize that the highest GMEAN score reached by an MAML model, that \textbf{incorporated} FP Growth, and taking the top 10 columns selected by feature selection. \textbf{The highest  BSS score is 0.84 shown in Table \ref{Tab:with_fp_top10_bss}}


\item We realize that the highest PR\_AUC score reached by an MAML model, that \textbf{incorporated} FP Growth, and taking the top 20 columns selected by feature selection. \textbf{The highest PR\_AUC score is 0.79 shown in Table \ref{Tab:with_fp_top20_bss}}
\end{enumerate}

%\subsection{Insights - Compare \& Contrast Meta Learners vs Shallow Models}
%
%\subsection*{F2-score - top 10 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_10_f2}}
%\caption{Best meta learners and shallow models in terms of f2 scores (results are sorted)}
%\label{Tab:top_10_f2}
%\end{table}
%
%\subsection*{F2-score - top 20 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_20_f2}}
%\caption{Best meta learners and shallow models in terms of f2 scores (results are sorted)}
%\label{Tab:top_20_f2}
%\end{table}
%
%\subsection*{BSS - top 10 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_10_bss}}
%\caption{Best meta learners and shallow models in terms of bss scores (results are sorted)}
%\label{Tab:top_10_bss}
%\end{table}
%
%\subsection*{BSS - top 20 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_20_bss}}
%\caption{Best meta learners and shallow models in terms of bss scores (results are sorted)}
%\label{Tab:top_20_bss}
%\end{table}
%
%\subsection*{GMEAN - top 10 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_10_gmean}}
%\caption{Best meta learners and shallow models in terms of gmean scores (results are sorted)}
%\label{Tab:top_10_gmean}
%\end{table}
%
%\subsection*{GMEAN - top 20 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_20_gmean}}
%\caption{Best meta learners and shallow models in terms of gmean scores (results are sorted)}
%\label{Tab:top_20_gmean}
%\end{table}
%
%\subsection*{PR\_AUC - top 10 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_10_pr_auc}}
%\caption{Best meta learners and shallow models in terms of pr\_auc scores (results are sorted)}
%\label{Tab:top_10_pr_auc}
%\end{table}
%
%\subsection*{PR\_AUC - top 20 columns}
%\begin{table}[H]
%\input{\detokenize{tables/top_20_pr_auc}}
%\caption{Best meta learners and shallow models in terms of pr\_auc scores (results are sorted)}
%\label{Tab:top_20_pr_auc}
%\end{table}
%
%We realize that in terms of quantitative results, meta learners always score better than the shallow models (except in table \ref{Tab:top_20_gmean}) the shallow models have slightly higher GMEAN

\subsection{Insights on With FP vs. Without FP}
In terms of error metrics, the differences are marginal between the experiments incorporating and those not incorporating FP growth. This observation is expected because, in both cases, we have modified the randomness in generating tasks for training the MAML model, in such a way that in both we made sure to generate tasks from all the heterogeneous data samples found in the dataset. However, although this is just a \textit{'heuristic'}, but it turns out to capture enough of the data.  \textbf{This heuristic covers tasks from all around the dataset in a \textit{round robin fashion}}

\subsection{Insights about PPV}
Its is important to note that in all of the results, we realize that PPV (\textbf{P}robability \textbf{P}ositi\textbf{V}e Class) was always $\geq 0.9$. This means that these models were able to assign high risks to some of the instances, which means that they were able to assign high risks to patients that are most likely to be demented. This effect will be captured in the \textbf{empirical risk curves}

\subsection{Qualitative Results}

\subsubsection{Mean Empirical Risk Curves}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top10/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top 6 models - Without FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top20/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top 6 models - Without FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Mear Empirical Risks for top models - Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top10/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top 6 models - With FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top20/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top 6 models - With FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Mean Empirical Risks for top models - With and Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top \textbf{shallow} models - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top \textbf{shallow} models - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Mean Empirical Risks for top models - With and Without FP - as well as shallow models trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}


In order to produce empirical risk curves, first, we rank patients by descending order of their estimated risk scores. We then group patients into bins based on the percentiles they fall into when categorized using risk scores. In our experiments, we choose to create 10 bins. The bottom 10\% of patients who have the least risk are grouped into a single bin. Those who rank between 10th and 20th percentile are grouped in the next bin and so on. For each such bin, we compute the \textit{empirical risk score} \textbf{which is the fraction of patients from that bin who actually, as per ground truth, are demented}. A good model would be classifying patients correctly if the \textit{empirical risk curve} is monotonically non-decreasing.

If the empirical risk curve is non-monotonic for some models, it implies that the classification using the model's risk scores may result in scenarios where patients with lower risk
scores are more likely to be demented compared to
patients with higher risk scores.

In the plots above, we realize that, whether with FP or without FP, the models that are trained using top 20 features have better empirical risk curves than those trained using top 10 features. It is also worth noting that although some models are non-monotonic, some are. \textbf{More importantly}, the shallow models are worse than meta learners, whereby their mean empirical risks decrease with the increase of risk, while the perfect behaviour must be exactly the opposite.

\subsubsection{Precision at Top K}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top10/precisions_topK.png}}
  \caption{Precisions for top 6 models - Without FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top20/precisions_topK.png}}
  \caption{Precisions at Top K for top 6 models - Without FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Precisions at Top K for top models - Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top10/precisions_topK.png}}
  \caption{Precisions at Top K for top 6 models - With FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top20/precisions_topK.png}}
  \caption{Precisions at Top K for top 6 models - With FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Precisions at Top K for top models - With and Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/precisions_topK.png}}
  \caption{Precisions at Top K for top \textbf{shallow} models - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_20/probabilistic/f2/precisions_topK.png}}
  \caption{Precisions at Top K for top \textbf{shallow} models - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Precisions at Top K for top models - With and Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}



It might happen that hospitals, for example, might want to admit only a certain number of patients, and for that, it might want to admit only the patients that are at a very high risk of having dementia. For that reason, clinicians might be interested in models that provide good risk estimates to rank . Therefore, it might be very helpful to provide the precision/recall values of various models at different values of K.

It is good to note that in the first 4 figures above (\textbf{meta learning}), the precision values do not drop a lot for smaller K values, for some models.

It is also important to note the last 2 figures (\textbf{shallow}), the models trained on top 20 columns perform good, as their precision (except for the Weighted Logistic Regression model) remain nearly constant for small values of $K$. 

\subsubsection{Recall at Top K}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top10/recalls_topK.png}}
  \caption{Recalls for top 6 models - Without FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top20/recalls_topK.png}}
  \caption{Recalls at Top K for top 6 models - Without FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Recalls at Top K for top models - Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top10/recalls_topK.png}}
  \caption{Recalls at Top K for top 6 models - With FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top20/recalls_topK.png}}
  \caption{Recalls at Top K for top 6 models - With FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Recalls at Top K for top models - With and Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/recalls_topK.png}}
  \caption{Recalls for top \textbf{shallow} models - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/recalls_topK.png}}
  \caption{Recalls at Top K for top \textbf{shallow} models - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Recalls at Top K for top models - Without FP - as well as shallow models trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}


We realize that the recall values at top K stay stable at 1.


\subsubsection{ROC Curves}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top10/roc_curves.png}}
  \caption{ROC Curves - Without FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/without_top20/roc_curves.png}}
  \caption{ROC Curves for top 6 models - Without FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{ROC Curves for top models - Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top10/roc_curves.png}}
  \caption{ROC Curves for top 6 models - With FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/maml/with_top20/roc_curves.png}}
  \caption{ROC Curves for top 6 models - With FP - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{ROC Curves for top models - With and Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/roc_curves.png}}
  \caption{ROC Curves for top \textbf{shallow} models - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/shallow/top_10/probabilistic/f2/roc_curves.png}}
  \caption{ROC Curves for top \textbf{shallow} models - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{ROC Curves for top \textbf{shallow} models trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

We realize that the best ROC Curves are those of the models trained on the top 20 features selected by feature selection, whether with or without FP. More importantly, the meta learners have much better roc curves than shallow models.


\section{ARML - Dementia}
\subsection{Quantitative Results - Dementia Top 10 Features}

\subsubsection*{Top 10 - F2 score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top10/dm_top10_f2}}}
\caption{Results - ARML, using top 10 columns - aggregated by f2 score}
\label{Tab:arml_top10_f2}
\end{table}

\subsubsection*{Top 10 - BSS score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top10/dm_top10_bss}}}
\caption{Results - ARML, using top 10 columns - aggregated by BSS score}
\label{Tab:arml_top10_bss}
\end{table}

\subsubsection*{Top 10 - GMEAN score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top10/dm_top10_gmean}}}
\caption{Results - ARML, using top 10 columns - aggregated by GMEAN score}
\label{Tab:arml_top10_gmean}
\end{table}

\subsubsection*{Top 10 - PR\_AUC score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top10/dm_top10_pr_auc}}}
\caption{Results - ARML, using top 10 columns - aggregated by PR\_AUC score}
\label{Tab:arml_top10_pr_auc}
\end{table}


\subsubsection*{Top 10 - Sensitivity score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top10/dm_top10_sensitivity}}}
\caption{Results - ARML, using top 10 columns - aggregated by Sensitivity score}
\label{Tab:arml_top10_sensitivity}
\end{table}

\subsubsection*{Top 10 - Specificity score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top10/dm_top10_specificity}}}
\caption{Results - ARML, using top 10 columns - aggregated by Specificity score}
\label{Tab:arml_top10_specificity}
\end{table}

\subsection{Quantitative Results - Dementia Top 20 Features}

\subsubsection*{Top 20 - F2 score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top20/dm_top20_f2}}}
\caption{Results - ARML, using top 20 columns - aggregated by f2 score}
\label{Tab:arml_top20_f2}
\end{table}

\subsubsection*{Top 10 - BSS score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top20/dm_top20_bss}}}
\caption{Results - ARML, using top 20 columns - aggregated by BSS score}
\label{Tab:arml_top20_bss}
\end{table}

\subsubsection*{Top 20 - GMEAN score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top20/dm_top20_gmean}}}
\caption{Results - ARML, using top 20 columns - aggregated by GMEAN score}
\label{Tab:arml_top20_gmean}
\end{table}

\subsubsection*{Top 20 - PR\_AUC score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top20/dm_top20_pr_auc}}}
\caption{Results - ARML, using top 20 columns - aggregated by PR\_AUC score}
\label{Tab:arml_top20_pr_auc}
\end{table}

\subsubsection*{Top 20 - Sensitivity score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top20/dm_top20_sensitivity}}}
\caption{Results - ARML, using top 20 columns - aggregated by Sensitivity score}
\label{Tab:arml_top20_sensitivity}
\end{table}

\subsubsection*{Top 20 - Specificity score}
% without fp - top 10 - f2
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/results_dm_top20/dm_top20_specificity}}}
\caption{Results - ARML, using top 20 columns - aggregated by Specificity score}
\label{Tab:arml_top20_specificity}
\end{table}

\subsection{Winning Hyper Parameters}
\subsubsection*{Top 10 Features}
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/hyperparameters/winning_top10_f2}}}
\caption{ARML - winning hyper parameters - top 10 features}
\label{Tab:arml_hyper_top10}
\end{table}

\subsubsection*{Top 20 Features}
\begin{table}[H]
\resizebox{.7\width}{!}{\input{\detokenize{tables/arml/hyperparameters/winning_top20_f2}}}
\caption{ARML - winning hyper parameters - top 20 features}
\label{Tab:arml_hyper_top20}
\end{table}

It is important to note that:
\begin{enumerate}
\item \textbf{step size $\alpha$:} is represented as the \textbf{update learning rate} (\textit{ulr}) in Tables \ref{Tab:arml_hyper_top10} and \ref{Tab:arml_hyper_top20} above.

\item \textbf{meta step size $\beta$:} is represented as the \textbf{meta learning rate} (\textit{mlr}) in Tables \ref{Tab:arml_hyper_top10} and \ref{Tab:arml_hyper_top20} above.

\item \textbf{K}: the number of samples $K$ denoting N-way K-shot classification is represented as the \textbf{update batch size} (\textit{ubs}) in Tables \ref{Tab:arml_hyper_top10} and \ref{Tab:arml_hyper_top20} above
\end{enumerate}


\subsection{Qualitative Results}

\subsubsection{Mean Empirical Risk Curves}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top10/mean_empirical_risks.png}}
  \caption{Mean Empirical Risks for top 6 models - ARML - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
\includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top20/mean_empirical_risks.png}}
 \caption{Mean Empirical Risks for top 6 models - ARML - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Mear Empirical Risks for top models - ARML - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}


In order to produce empirical risk curves, first, we rank patients by descending order of their estimated risk scores. We then group patients into bins based on the percentiles they fall into when categorized using risk scores. In our experiments, we choose to create 10 bins. The bottom 10\% of patients who have the least risk are grouped into a single bin. Those who rank between 10th and 20th percentile are grouped in the next bin and so on. For each such bin, we compute the \textit{empirical risk score} \textbf{which is the fraction of patients from that bin who actually, as per ground truth, are demented}. A good model would be classifying patients correctly if the \textit{empirical risk curve} is monotonically non-decreasing.

If the empirical risk curve is non-monotonic for some models, it implies that the classification using the model's risk scores may result in scenarios where patients with lower risk
scores are more likely to be demented compared to
patients with higher risk scores.

In the plots above, we realize that,the models that are trained using top 20 features have better empirical risk curves than those trained using top 10 features.

\subsubsection{Precision at Top K}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top10//precisions_topK.png}}
  \caption{Precisions for top 6 models - ARML - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top20//precisions_topK.png}}
  \caption{Precisions at Top K for top 6 models - ARML - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Precisions at Top K for top models - ARML - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

It might happen that hospitals, for example, might want to admit only a certain number of patients, and for that, it might want to admit only the patients that are at a very high risk of having dementia. For that reason, clinicians might be interested in models that provide good risk estimates to rank . Therefore, it might be very helpful to provide the precision/recall values of various models at different values of K.

It is also important to note that the models trained on top 20 columns perform good, as their precision remain nearly constant for small values of $K$.

\subsubsection{Recall at Top K}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top10/recalls_topK.png}}
  \caption{Recalls for top 6 models - ARML - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top20/recalls_topK.png}}
  \caption{Recalls at Top K for top 6 models - ARML - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{Recalls at Top K for top models - ARML - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}

We realize that the recall values at top K stay stable at 1 for some models, both the ones trained on top 10 and the ones trained on top 20.


\subsubsection{ROC Curves}
\begin{figure}[H]
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top10/roc_curves.png}}
  \caption{ROC Curves - Without FP - top 10 columns}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{\detokenize{advanced_ml_plots/arml/dm_top20/roc_curves.png}}
  \caption{ROC Curves for top 6 models - ARML - top 20 columns}
  \label{fig:sub2}
\end{subfigure}
\caption{ROC Curves for top models - Without FP - trained on top 10 and top 20 columns selected by feature selection}
\label{fig:test}
\end{figure}


We realize that the best ROC Curves are those of the models trained on the top 20 features selected by feature selection.

\subsection{Frequent Patterns and Probability of Mistakes}
\url{https://drive.google.com/drive/folders/1eoIYwVf7vo4haKFwJV779pwVYC2ddgxZ?usp=sharing}


\section{All Results - Dementia - Quantitative}
\begin{figure}[H]
\centering
\includegraphics[scale=0.2]{\detokenize{all_results}}
\caption{Best Results of all methods on dementia dataset}
\end{figure}

Although the MAML on top 20 achieves the highest f2 score, but we realize that the ARML model on top 20 might be a better classifier, because it is high on all metrics, while MAML was low on BSS and specificity.


\section{Reptile}
\subsection{Hyper Parameters}
\begin{table}[H]
%\resizebox{.7\width}{!}
{\input{\detokenize{tables/reptile/hyperparameters/meta_learning_hyperparameters}}}
\caption{Hyper Parameter Space Containing all Meta-Learning Related Hyper Parameters}
\label{Tab:reptile_meta_learning_hyperparameters}
\end{table}

\end{document}